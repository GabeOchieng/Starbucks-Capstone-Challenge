{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks Capstone Challenge: Using Starbucks app user data to predict effective offers\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In my capstone project, I aim to answer 2 main business questions:\n",
    "1. What are the main drivers of an effective offer on the Starbucks app? \n",
    "2. Could the data provided, namely offer characteristics and user demographics, predict whether a user would take up an offer? \n",
    "\n",
    "This capstone project is using data provided by Udacity as part of the Data Scientist Nanodegree course. It contains simulated data that mimics customer behavior on the Starbucks rewards mobile app.\n",
    "\n",
    "The provided background information on the mobile app is that once every few days, Starbucks sends out an offer to users of the mobile app. Some users might not receive any offer during certain weeks, and not all users receive the same offer.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "As stated above, the problem statement I am aiming to answer are to (1) discover the main drivers of offer effectiveness, and (2) explore if we can predict whether a user would take up an offer.\n",
    "\n",
    "The data provided consists of 3 datasets:\n",
    "- Offer portfolio, which consists of the attributes of each offer\n",
    "- Demographic data for each customer\n",
    "- Transactional records of events occurring on the app\n",
    "\n",
    "Using the data provided, I answer the above two questions using 3 classification supervised machine learning models, feeding in the data from three different offer types. \n",
    "\n",
    "I use the model to uncover the feature importances to identify the drivers of offer effectiveness, while exploring if the model itself could be used to predict if a user would take up an offer. \n",
    "\n",
    "Lastly, I also explore the characteristics of users who do or do not take up an offer.\n",
    "\n",
    "My project aims to answer the two questions above, but I also ended up adding 2 additional models as points of exploration - the first assessing whether an all-in-one model could be used in place of 3 different models, with the offer types functioning as a categorical variable. Secondly, I also build a regression model to see if we could predict the amount a user would spend, given that the offer is effectively influencing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the json data\n",
    "portfolio = pd.read_json('./data/portfolio.json', orient='records', lines=True)\n",
    "profile = pd.read_json('./data/profile.json', orient='records', lines=True)\n",
    "transcript = pd.read_json('./data/transcript.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "### a. Offer portfolio data\n",
    "\n",
    "According to the information provided by Udacity, the schema is as follows:\n",
    "\n",
    "**portfolio.json**\n",
    "* id (string) - offer id\n",
    "* offer_type (string) - type of offer ie BOGO, discount, informational\n",
    "* difficulty (int) - minimum required spend to complete an offer\n",
    "* reward (int) - reward given for completing an offer\n",
    "* duration (int) - \n",
    "* channels (list of strings)\n",
    "\n",
    "Moreover, some further information given about the offers is that there are 3 different offer types:\n",
    "- BOGO - buy one get one free\n",
    "- Discount - discount with purchase\n",
    "- Informational - provides information about products\n",
    "\n",
    "Thus, the schema is pretty straightforward, as it contains the attributes of 3 different offer types. While the duration was not explained I assumed from context that it is in terms of number of days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_characterization(data):\n",
    "    print(\"shape of data : \"+str(data.shape))\n",
    "    data_characterization=pd.DataFrame()\n",
    "    columns=data.columns\n",
    "    Type=[]\n",
    "    Count=[]\n",
    "    unique_values=[]\n",
    "    Max=[]\n",
    "    Min=[]\n",
    "    Mean=[]\n",
    "    Nan_counts=data.isnull().sum().tolist()\n",
    "    Nan_ratio=(data.isnull().sum()/len(data)).values\n",
    "\n",
    "    Type=data.dtypes.tolist()\n",
    "    J=0\n",
    "    for  i  in columns : \n",
    "       \n",
    "        \n",
    "        \n",
    "        if (data[i].dtypes.name == 'object') :\n",
    "            Max.append(0)\n",
    "            Min.append(0)\n",
    "            unique_values.append([])\n",
    "            Count.append(0)\n",
    "            Mean.append(0)\n",
    "        elif ( (data[i].dtypes == '<M8[ns]') ) : \n",
    "            Max.append(0)\n",
    "            Min.append(0)\n",
    "            unique=list(data[i].unique())\n",
    "            unique_values.append(unique)\n",
    "            Count.append(0)\n",
    "            Mean.append(0)\n",
    "        elif ( (data[i].dtype.name==\"category\") ) : \n",
    "            Max.append(0)\n",
    "            Min.append(0)\n",
    "            unique=list(data[i].unique())\n",
    "            unique_values.append(unique)\n",
    "            Count.append(0)\n",
    "            Mean.append(0)\n",
    "\n",
    "        else : \n",
    "            unique=list(data[i].unique())\n",
    "            unique_values.append(unique)\n",
    "            Count.append(len(unique))\n",
    "            Max.append(data[i].max())\n",
    "            Min.append(data[i].min())\n",
    "            Mean.append(data[i].mean())\n",
    "   \n",
    "    data_characterization[\"Columns name\"]=columns\n",
    "    data_characterization[\"Type \"]=data.dtypes.tolist()\n",
    "    data_characterization[\"Count unique values\"]=Count\n",
    "    data_characterization[\"Count Nan values\"]=Nan_counts\n",
    "    data_characterization[\"Ratio Nan values\"]=Nan_ratio\n",
    "\n",
    "    data_characterization[\"Unique   values\"]=unique_values\n",
    "    data_characterization[\"Max\"]=Max\n",
    "    data_characterization[\"Min\"]=Min\n",
    "    data_characterization[\"Mean\"]=Mean\n",
    "    \n",
    "    display(data_characterization)    \n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channels</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>duration</th>\n",
       "      <th>id</th>\n",
       "      <th>offer_type</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[email, mobile, social]</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>ae264e3637204a6fb9bb56bc8210ddfd</td>\n",
       "      <td>bogo</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[web, email, mobile, social]</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>4d5c57ea9a6940dd891ad53e9dbe8da0</td>\n",
       "      <td>bogo</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[web, email, mobile]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>informational</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[web, email, mobile]</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>9b98b8c7a33c4b65b9aebfe6a799e6d9</td>\n",
       "      <td>bogo</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[web, email]</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0b1e1539f2cc45b7b9fa7c272da2e1d7</td>\n",
       "      <td>discount</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       channels  difficulty  ...     offer_type reward\n",
       "0       [email, mobile, social]          10  ...           bogo     10\n",
       "1  [web, email, mobile, social]          10  ...           bogo     10\n",
       "2          [web, email, mobile]           0  ...  informational      0\n",
       "3          [web, email, mobile]           5  ...           bogo      5\n",
       "4                  [web, email]          20  ...       discount      5\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data : (10, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columns name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Count unique values</th>\n",
       "      <th>Count Nan values</th>\n",
       "      <th>Ratio Nan values</th>\n",
       "      <th>Unique   values</th>\n",
       "      <th>Max</th>\n",
       "      <th>Min</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>channels</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>difficulty</td>\n",
       "      <td>int64</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[10, 0, 5, 20, 7]</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>duration</td>\n",
       "      <td>int64</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[7, 5, 4, 10, 3]</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>offer_type</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reward</td>\n",
       "      <td>int64</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[10, 0, 5, 3, 2]</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Columns name   Type   Count unique values  ...  Max  Min Mean\n",
       "0     channels  object                    0  ...    0    0  0.0\n",
       "1   difficulty   int64                    5  ...   20    0  7.7\n",
       "2     duration   int64                    5  ...   10    3  6.5\n",
       "3           id  object                    0  ...    0    0  0.0\n",
       "4   offer_type  object                    0  ...    0    0  0.0\n",
       "5       reward   int64                    5  ...   10    0  4.2\n",
       "\n",
       "[6 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_characterization(portfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `channels` column consists of nested lists. Hence, I note that I will have to expand the column later during preprocessing to become categorical variables in my dataset. I also note that the scale of each are different, for example the `difficulty` is in terms of dollars while the `duration` is in terms of days. Hence, some feature scaling will need to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "channels      0\n",
       "difficulty    0\n",
       "duration      0\n",
       "id            0\n",
       "offer_type    0\n",
       "reward        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values\n",
    "portfolio.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good news is that there are no missing values in this dataset, hence we won't have to impute or make any decisions to remove them during the preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check no. of unique offers\n",
    "portfolio.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "offer_type\n",
       "bogo             4\n",
       "discount         4\n",
       "informational    2\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio.groupby('offer_type')['id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 unique offer ids we will have to take note of, with 4 each of bogo and discount types, while 2 informational types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Demographic data\n",
    "\n",
    "Demographic data for customers is provided in the `profile` dataset. The schema and variables are as follows: \n",
    "\n",
    "**profile.json**\n",
    "* age (int) - age of the customer \n",
    "* became_member_on (int) - date when customer created an app account\n",
    "* gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n",
    "* id (str) - customer id\n",
    "* income (float) - customer's income\n",
    "\n",
    "It is also relatively straightforward, as it contains the demographic profile on the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>became_member_on</th>\n",
       "      <th>gender</th>\n",
       "      <th>id</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118</td>\n",
       "      <td>20170212</td>\n",
       "      <td>None</td>\n",
       "      <td>68be06ca386d4c31939f3a4f0e3dd783</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>20170715</td>\n",
       "      <td>F</td>\n",
       "      <td>0610b486422d4921ae7d2bf64640c50b</td>\n",
       "      <td>112000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>20180712</td>\n",
       "      <td>None</td>\n",
       "      <td>38fe809add3b4fcf9315a9694bb96ff5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75</td>\n",
       "      <td>20170509</td>\n",
       "      <td>F</td>\n",
       "      <td>78afa995795e4d85b5d9ceeca43f5fef</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118</td>\n",
       "      <td>20170804</td>\n",
       "      <td>None</td>\n",
       "      <td>a03223e636434f42ac4c3df47e8bac43</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  became_member_on gender                                id    income\n",
       "0  118          20170212   None  68be06ca386d4c31939f3a4f0e3dd783       NaN\n",
       "1   55          20170715      F  0610b486422d4921ae7d2bf64640c50b  112000.0\n",
       "2  118          20180712   None  38fe809add3b4fcf9315a9694bb96ff5       NaN\n",
       "3   75          20170509      F  78afa995795e4d85b5d9ceeca43f5fef  100000.0\n",
       "4  118          20170804   None  a03223e636434f42ac4c3df47e8bac43       NaN"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data : (17000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columns name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Count unique values</th>\n",
       "      <th>Count Nan values</th>\n",
       "      <th>Ratio Nan values</th>\n",
       "      <th>Unique   values</th>\n",
       "      <th>Max</th>\n",
       "      <th>Min</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>int64</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[118, 55, 75, 68, 65, 58, 61, 26, 62, 49, 57, ...</td>\n",
       "      <td>118.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.253141e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>became_member_on</td>\n",
       "      <td>int64</td>\n",
       "      <td>1716</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[20170212, 20170715, 20180712, 20170509, 20170...</td>\n",
       "      <td>20180726.0</td>\n",
       "      <td>20130729.0</td>\n",
       "      <td>2.016703e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>2175</td>\n",
       "      <td>0.127941</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>income</td>\n",
       "      <td>float64</td>\n",
       "      <td>92</td>\n",
       "      <td>2175</td>\n",
       "      <td>0.127941</td>\n",
       "      <td>[nan, 112000.0, 100000.0, 70000.0, 53000.0, 51...</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>6.540499e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Columns name    Type   ...         Min          Mean\n",
       "0               age    int64  ...        18.0  6.253141e+01\n",
       "1  became_member_on    int64  ...  20130729.0  2.016703e+07\n",
       "2            gender   object  ...         0.0  0.000000e+00\n",
       "3                id   object  ...         0.0  0.000000e+00\n",
       "4            income  float64  ...     30000.0  6.540499e+04\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_characterization(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first 5 lines we can already see some null values in `gender` and `income`, while the `age` column contains some values that don't make sense (e.g. 118)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                    0\n",
       "became_member_on       0\n",
       "gender              2175\n",
       "id                     0\n",
       "income              2175\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values\n",
    "profile.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, the `gender` and `income` have null values. Some good news is that whichever values are null in `gender` are also null in `income`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0bd3baec18>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check distribution of age column\n",
    "profile.age.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the `age = 118` value does not make sense there as it is clearly out of the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 2175\n",
      "became_member_on    2175\n",
      "gender                 0\n",
      "id                  2175\n",
      "income                 0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  income  age\n",
       "0   None     NaN  118\n",
       "2   None     NaN  118\n",
       "4   None     NaN  118\n",
       "6   None     NaN  118\n",
       "7   None     NaN  118"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check count of age=118 value and corresponding gender and income columns\n",
    "print(profile[profile['age']==118].count())\n",
    "profile[['gender','income','age']][profile['age']==118].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the `age=118` column corresponds with the null `gender` and `income` columns. Thus, we can actually drop them during preprocessing if they do not take too large a proportion of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how many unique people are in dataset\n",
    "profile['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0bd3159978>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check distributions of income\n",
    "profile.income.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, the `became_member_on` column has some potential to be feature engineered to get the tenure of membership in days. This feature might have some influence on whether an offer is effective or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20170212\n",
       "1    20170715\n",
       "2    20180712\n",
       "3    20170509\n",
       "4    20170804\n",
       "Name: became_member_on, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile.became_member_on.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Transactional records\n",
    "\n",
    "The schema for the transactional data is as follows:\n",
    "\n",
    "**transcript.json**\n",
    "* event (str) - record description (ie transaction, offer received, offer viewed, etc.)\n",
    "* person (str) - customer id\n",
    "* time (int) - time in hours. The data begins at time t=0\n",
    "* value - (dict of strings) - either an offer id or transaction amount depending on the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>person</th>\n",
       "      <th>time</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>offer received</td>\n",
       "      <td>78afa995795e4d85b5d9ceeca43f5fef</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': '9b98b8c7a33c4b65b9aebfe6a799e6d9'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>offer received</td>\n",
       "      <td>a03223e636434f42ac4c3df47e8bac43</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': '0b1e1539f2cc45b7b9fa7c272da2e1d7'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>offer received</td>\n",
       "      <td>e2127556f4f64592b11af22de27a7932</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': '2906b810c7d4411798c6938adc9daaa5'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>offer received</td>\n",
       "      <td>8ec6ce2a7e7949b1bf142def7d0e0586</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': 'fafdcd668e3743c1bb461111dcafc2a4'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>offer received</td>\n",
       "      <td>68617ca6246f4fbc85e91a2a49552598</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': '4d5c57ea9a6940dd891ad53e9dbe8da0'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            event  ...                                             value\n",
       "0  offer received  ...  {'offer id': '9b98b8c7a33c4b65b9aebfe6a799e6d9'}\n",
       "1  offer received  ...  {'offer id': '0b1e1539f2cc45b7b9fa7c272da2e1d7'}\n",
       "2  offer received  ...  {'offer id': '2906b810c7d4411798c6938adc9daaa5'}\n",
       "3  offer received  ...  {'offer id': 'fafdcd668e3743c1bb461111dcafc2a4'}\n",
       "4  offer received  ...  {'offer id': '4d5c57ea9a6940dd891ad53e9dbe8da0'}\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data : (306534, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columns name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Count unique values</th>\n",
       "      <th>Count Nan values</th>\n",
       "      <th>Ratio Nan values</th>\n",
       "      <th>Unique   values</th>\n",
       "      <th>Max</th>\n",
       "      <th>Min</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>event</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>person</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>time</td>\n",
       "      <td>int64</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66,...</td>\n",
       "      <td>714</td>\n",
       "      <td>0</td>\n",
       "      <td>366.38294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>value</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Columns name   Type   Count unique values  ...  Max  Min       Mean\n",
       "0        event  object                    0  ...    0    0    0.00000\n",
       "1       person  object                    0  ...    0    0    0.00000\n",
       "2         time   int64                  120  ...  714    0  366.38294\n",
       "3        value  object                    0  ...    0    0    0.00000\n",
       "\n",
       "[4 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_characterization(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['offer received', 'offer viewed', 'transaction', 'offer completed'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript.event.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data looks a bit more tricky, as it is ordered by time and has an event and value. In particular, the `value` column will have to be preprocessed depending on the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check number of unique people represented\n",
    "transcript['person'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks as though the number of people in transcript are the same as the number of people in the Demographics Data, so that is good news. But a lot of preprocessing will need to be done in order to extract meaningful insights out of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event     0\n",
       "person    0\n",
       "time      0\n",
       "value     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values\n",
    "transcript.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values in this data.\n",
    "\n",
    "In order to extract insights from the value column, I will have to expand the values into individual columns depending on the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>person</th>\n",
       "      <th>time</th>\n",
       "      <th>value</th>\n",
       "      <th>offer id</th>\n",
       "      <th>amount</th>\n",
       "      <th>offer_id</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>offer received</td>\n",
       "      <td>78afa995795e4d85b5d9ceeca43f5fef</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': '9b98b8c7a33c4b65b9aebfe6a799e6d9'}</td>\n",
       "      <td>9b98b8c7a33c4b65b9aebfe6a799e6d9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>offer received</td>\n",
       "      <td>a03223e636434f42ac4c3df47e8bac43</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': '0b1e1539f2cc45b7b9fa7c272da2e1d7'}</td>\n",
       "      <td>0b1e1539f2cc45b7b9fa7c272da2e1d7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>offer received</td>\n",
       "      <td>e2127556f4f64592b11af22de27a7932</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': '2906b810c7d4411798c6938adc9daaa5'}</td>\n",
       "      <td>2906b810c7d4411798c6938adc9daaa5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>offer received</td>\n",
       "      <td>8ec6ce2a7e7949b1bf142def7d0e0586</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': 'fafdcd668e3743c1bb461111dcafc2a4'}</td>\n",
       "      <td>fafdcd668e3743c1bb461111dcafc2a4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>offer received</td>\n",
       "      <td>68617ca6246f4fbc85e91a2a49552598</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': '4d5c57ea9a6940dd891ad53e9dbe8da0'}</td>\n",
       "      <td>4d5c57ea9a6940dd891ad53e9dbe8da0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            event                            person  ...  offer_id reward\n",
       "0  offer received  78afa995795e4d85b5d9ceeca43f5fef  ...       NaN    NaN\n",
       "1  offer received  a03223e636434f42ac4c3df47e8bac43  ...       NaN    NaN\n",
       "2  offer received  e2127556f4f64592b11af22de27a7932  ...       NaN    NaN\n",
       "3  offer received  8ec6ce2a7e7949b1bf142def7d0e0586  ...       NaN    NaN\n",
       "4  offer received  68617ca6246f4fbc85e91a2a49552598  ...       NaN    NaN\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript=pd.concat([transcript, transcript['value'].apply(pd.Series)], axis=1)\n",
    "transcript.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though the `offer id` column ended up being duplicates so we have to clean it up further to ensure there is only one `offer id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column to ensure only one offer_id column\n",
    "transcript['offer_id_new']=np.where(transcript['offer id'].isnull() & transcript['offer_id'].notnull(),transcript['offer_id'],transcript['offer id'])\n",
    "\n",
    "#drop unnecessary offer_id columns\n",
    "transcript.drop(['offer id','offer_id'],axis=1,inplace=True)\n",
    "\n",
    "#rename offer_id column\n",
    "transcript.rename(columns={'offer_id_new':'offer_id'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>person</th>\n",
       "      <th>time</th>\n",
       "      <th>value</th>\n",
       "      <th>amount</th>\n",
       "      <th>reward</th>\n",
       "      <th>offer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>offer received</td>\n",
       "      <td>78afa995795e4d85b5d9ceeca43f5fef</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': '9b98b8c7a33c4b65b9aebfe6a799e6d9'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9b98b8c7a33c4b65b9aebfe6a799e6d9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>offer received</td>\n",
       "      <td>a03223e636434f42ac4c3df47e8bac43</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': '0b1e1539f2cc45b7b9fa7c272da2e1d7'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0b1e1539f2cc45b7b9fa7c272da2e1d7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>offer received</td>\n",
       "      <td>e2127556f4f64592b11af22de27a7932</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': '2906b810c7d4411798c6938adc9daaa5'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2906b810c7d4411798c6938adc9daaa5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>offer received</td>\n",
       "      <td>8ec6ce2a7e7949b1bf142def7d0e0586</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': 'fafdcd668e3743c1bb461111dcafc2a4'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fafdcd668e3743c1bb461111dcafc2a4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>offer received</td>\n",
       "      <td>68617ca6246f4fbc85e91a2a49552598</td>\n",
       "      <td>0</td>\n",
       "      <td>{'offer id': '4d5c57ea9a6940dd891ad53e9dbe8da0'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4d5c57ea9a6940dd891ad53e9dbe8da0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            event  ...                          offer_id\n",
       "0  offer received  ...  9b98b8c7a33c4b65b9aebfe6a799e6d9\n",
       "1  offer received  ...  0b1e1539f2cc45b7b9fa7c272da2e1d7\n",
       "2  offer received  ...  2906b810c7d4411798c6938adc9daaa5\n",
       "3  offer received  ...  fafdcd668e3743c1bb461111dcafc2a4\n",
       "4  offer received  ...  4d5c57ea9a6940dd891ad53e9dbe8da0\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the additional transcript columns can be used for further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining approach for preprocessing data for the model\n",
    "\n",
    "Before I proceeded to preprocess the data for the model, I first revisited my objective. Having done a preliminary exploration of the data, I had to reassess how I would clean and prepare the data for the models I intended to build.\n",
    "\n",
    "In order to identify the main drivers of an effective offer, I have to first define what an 'effective' offer is within the Starbucks app. Thus, I did some further exploration on the datasets and how all three would interact.\n",
    "\n",
    "First, I had to explore what kind of events are within each offer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column\n",
    "portfolio.rename(columns={'id':'offer_id'},inplace=True)\n",
    "\n",
    "#join transcript with offer type\n",
    "transcript=transcript.merge(portfolio,how='left',on='offer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "event            offer_type   \n",
       "offer completed  bogo             15669\n",
       "                 discount         17910\n",
       "offer received   bogo             30499\n",
       "                 discount         30543\n",
       "                 informational    15235\n",
       "offer viewed     bogo             25449\n",
       "                 discount         21445\n",
       "                 informational    10831\n",
       "Name: offer_type, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript.groupby(['event','offer_type'])['offer_type'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that there are 4 types of events: `offer completed`, `offer received`, `offer viewed` and `transaction`. But our data shows that we do not have any offer_id associated with `transactions`, because they are not recorded in the transcript event data. Thus, the first objective in data preprocessing is to define a methodology to assign offer_ids to specific transactions.\n",
    "\n",
    "Moreover, we also know that BOGO and discount offers have an `offer completed` event when offers are completed. However, informational offers do not have this event associated with it. Thus, we also specify the approach to define an effective offer as follows:\n",
    "\n",
    "For a BOGO and discount offer, an effective offer would be defined if the following events were recorded in the right sequence in time:\n",
    "\n",
    "`offer received` -> `offer viewed` -> `transaction` -> `offer completed`\n",
    "\n",
    "Meanwhile, for an informational offer, since there `offer completed` event associated with it, I will have to define transactions as a conversion to effective offer:\n",
    "\n",
    "`offer received` -> `offer viewed` -> `transaction`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### a. Assigning offer ids to transactions\n",
    "\n",
    "After defining the approach above, we now have to explore methods to assign offer_ids to specific transactions. Among the \n",
    "considerations is to define the following main groups of customers:\n",
    "\n",
    "**1. People who are influenced and successfully convert - effective offers:**\n",
    "\n",
    "    - `offer received` -> `offer viewed` -> `transaction` -> `offer completed` (BOGO/discount offers)\n",
    "    - `offer received` -> `offer viewed` -> `transaction` (informational offers - must be within validity period of offer)\n",
    "\n",
    "**2. People who received and viewed an offer but did not successfully convert - ineffective offers:**\n",
    "\n",
    "    - `offer received` -> `offer viewed`\n",
    "    \n",
    "**3. People who purchase/complete offers regardless of awareness of any offers:**\n",
    "    \n",
    "    - `transaction`\n",
    "    - `offer received` -> `transaction` -> `offer completed` -> `offer viewed`\n",
    "    - `transaction` -> `offer received` -> `offer completed` -> `offer viewed`\n",
    "    - `offer received` -> `transaction` -> `offer viewed` -> `offer completed`\n",
    "    - `offer received` -> `transaction` (informational offers)\n",
    "    - `offer received` -> `transaction` -> `offer viewed` (informational offers)\n",
    "\n",
    "**4. People who received offers but no action taken:**\n",
    "\n",
    "    - `offer received`\n",
    "    \n",
    "For people in group 2, I would need to check if there are events where there is an `offer received` and `offer viewed` event, but no conversion event, i.e. `offer completed` or `transaction` - these are cases of ineffective offers.\n",
    "\n",
    "I would have to separate out the people in group 2 from people in group 4, as people in group 2 may have viewed an offer but did not take any action, whereas people in group 4 did not even have an `offer viewed` event.\n",
    "\n",
    "Separating the conversions for effective offers (group 1) and people who purchase/complete offers regardless of awareness of any offers (group 3) is particularly tricky. \n",
    "For people in group 3, a conversion is invalid (i.e., not a successful conversion from an offer) if an `offer completed` or `transaction` occurs before an `offer viewed`. There also may be scenarios where an `offer completed` occurs after the offer is viewed, but a transaction was done prior to the offer being viewed. In this instance, the offer may have been completed, but it is also not a valid conversion.  \n",
    "\n",
    "**Defining the target variable `effective offer`:**\n",
    "\n",
    "After defining these conditions, we have to decide what the target variable will be.\n",
    "\n",
    "We know that group 1 customers will be our target variable `effective_offer=1`, but there are many ineffective offer definitions for groups 2-4. \n",
    "\n",
    "So what would we define as an ineffective offer? As already stated above, group 2 would be within our definition of an ineffective offer; where a user is aware of an offer, but the offer is ineffective as it does not convert the user into a customer. So group 2 can be defined as our target variable `effective_offer=0`.\n",
    "\n",
    "What about group 3 and group 4? Group 3 consists of users who may have received offers but would have purchased regardless. From the business point of view, we would not want to be sending them any offers. \n",
    "\n",
    "Meanwhile, group 4 users would be considered low priority customers, as they do not do any action, regardless of whether they receive offers or not.  \n",
    "\n",
    "So, we can deprioritise group 3 and group 4 users from our model. It would still be worth doing some exploratory analysis onto group 3 and 4, just to explore on their demographics.\n",
    "\n",
    "The conditions above are the basis of which I can assign the offer id that 'influences' a transaction by ensuring that the transaction occurs after an `offer viewed` event. \n",
    "\n",
    "After sorting the transcript dataset by person and time to ensure that each event for each person occurs in sequence, I can filter the dataset by events `offer viewed` and `transaction` to ensure that it only contains those events in order. \n",
    "\n",
    "Then, I can use pandas' `ffill()` method to fill every transaction with the offer_id of the viewed offer, only if it occurs before the transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dropcolumns function as I will be doing this many times\n",
    "def drop_cols(drop_cols,df,inplace=False):\n",
    "    '''\n",
    "    inputs:\n",
    "    - drop_cols: list or string of column name to be dropped\n",
    "    - df: dataframe from which column should be dropped\n",
    "    - inplace: specify whether columns are dropped in place or not\n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe with dropped columns.\n",
    "       \n",
    "    '''\n",
    "    df=df.drop(columns=drop_cols,axis=1,inplace=inplace)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary columns to clean dataset\n",
    "transcript=drop_cols(['reward_x','reward_y'],transcript)\n",
    "#sort events by person and time\n",
    "transcript=transcript.sort_values(['person','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter dataset for transactions that occur after an offer is viewed, forward fill offer ids by person\n",
    "offers_view_transacted=transcript[['time','offer_id','person','event']][(transcript['event']=='transaction') | (transcript['event']=='offer viewed')].groupby(['person','offer_id']).ffill()\n",
    "offers_view_transacted['offer_id']=offers_view_transacted['offer_id'].ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the above temporary dataset is just a subset of the `transcript` dataset, I can create a new dataset with the filled in offer ids for transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript=transcript.merge(offers_view_transacted,how='left',on=['person','time','event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up dataset to unify multiple offer_id columns into one column\n",
    "transcript['offer_id']=np.where(transcript['offer_id_x'].isnull(),transcript['offer_id_y'],transcript['offer_id_x'])\n",
    "\n",
    "drop_cols(['offer_id_x','offer_id_y'],transcript,inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge portfolio dataset again to get offer data for the transaction events\n",
    "transcript=transcript.merge(portfolio,how='left',on='offer_id')\n",
    "transcript['duration']=np.where(transcript['duration_x'].isnull(),transcript['duration_y'],transcript['duration_x'])\n",
    "drop_cols(['duration_x','offer_type_x','difficulty_x','channels_x','duration_y'],transcript,inplace=True);\n",
    "transcript.rename(columns={'channels_y':'channels','reward_y':'reward','difficulty_y':'difficulty','offer_type_y':'offer_type'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>person</th>\n",
       "      <th>time</th>\n",
       "      <th>value</th>\n",
       "      <th>amount</th>\n",
       "      <th>offer_id</th>\n",
       "      <th>channels</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>offer_type</th>\n",
       "      <th>reward</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>offer received</td>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>168</td>\n",
       "      <td>{'offer id': '5a8bc65990b245e5a138643cd4eb9837'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>[email, mobile, social]</td>\n",
       "      <td>0</td>\n",
       "      <td>informational</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>offer viewed</td>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>192</td>\n",
       "      <td>{'offer id': '5a8bc65990b245e5a138643cd4eb9837'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>[email, mobile, social]</td>\n",
       "      <td>0</td>\n",
       "      <td>informational</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transaction</td>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>228</td>\n",
       "      <td>{'amount': 22.16}</td>\n",
       "      <td>22.16</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>[email, mobile, social]</td>\n",
       "      <td>0</td>\n",
       "      <td>informational</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>offer received</td>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>336</td>\n",
       "      <td>{'offer id': '3f207df678b143eea3cee63160fa8bed'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>[web, email, mobile]</td>\n",
       "      <td>0</td>\n",
       "      <td>informational</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>offer viewed</td>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>372</td>\n",
       "      <td>{'offer id': '3f207df678b143eea3cee63160fa8bed'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>[web, email, mobile]</td>\n",
       "      <td>0</td>\n",
       "      <td>informational</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            event                            person  ...  reward duration\n",
       "0  offer received  0009655768c64bdeb2e877511632db8f  ...       0      3.0\n",
       "1    offer viewed  0009655768c64bdeb2e877511632db8f  ...       0      3.0\n",
       "2     transaction  0009655768c64bdeb2e877511632db8f  ...       0      3.0\n",
       "3  offer received  0009655768c64bdeb2e877511632db8f  ...       0      4.0\n",
       "4    offer viewed  0009655768c64bdeb2e877511632db8f  ...       0      4.0\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Flagging transactions and offers completed after offers viewed\n",
    "\n",
    "The next important step for preparing our data for modeling and analysis is to identify a completed offer and transactions occurring after an offer is viewed. \n",
    "\n",
    "Once we have assigned a transaction occurring after an offer is viewed, I can use that information to subset my data according to the groups defined above, and analyse within each group.\n",
    "\n",
    "Using our dataset with the offer_ids populated for `transaction` events, we can  flag the converted transactions and completed offers. We have to first ensure that the offer id of the previous event is the same one. Since we have tagged the offer id for all viewed, transactions and completed offers, we can use the `offer_id` field to ensure that the previous offer consists of those events. \n",
    "\n",
    "This means that as long as the events `offer viewed`,`transaction`, and `offer completed` occur in the same event space and are in the corrrect sequence of time, we can be assured that it is a transaction and/or completed offer occurring only after an offer is viewed. \n",
    "\n",
    "To do this, I created a new column to flag the previous offer id using pandas' `shift` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sample space of events consisting of offer viewed, transactions and offer completed\n",
    "offers_viewed_transactions_completed=transcript[(transcript['event']=='offer viewed') | (transcript['event']=='transaction') | (transcript['event']=='offer completed')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add extra column to flag the previous offer id\n",
    "offers_viewed_transactions_completed['offer_id_previous'] = offers_viewed_transactions_completed.groupby(['person','offer_id'])['offer_id'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flag a completed transaction/offer completed as long as the previous offer id consists of events in the same sample space\n",
    "offers_viewed_transactions_completed['valid_completed']=np.where(offers_viewed_transactions_completed['offer_id_previous']==offers_viewed_transactions_completed['offer_id'],1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset `offers_viewed_transactions_completed` consists of all other possible events, all we need to do is to append the all `offers received` events in the `transactions_clean` dataset to ensure we have our complete dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get only offer received events\n",
    "offers_received=transcript[transcript['event']=='offer received'].copy()\n",
    "\n",
    "#ensure all columns are the same between datasets to be appended\n",
    "offers_received['offer_id_previous']=np.nan\n",
    "offers_received['valid_completed']=np.nan\n",
    "\n",
    "#append datasets to complete dataset of transactions\n",
    "transcript=offers_received.append(offers_viewed_transactions_completed)\n",
    "\n",
    "#sort values\n",
    "transcript=transcript.sort_values(['person','time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having assigned offer_ids for transactions for which an `offer viewed` event occurred prior, we can now revisit the four customer groups of unique person-offer_id pairs we are trying to analyse.\n",
    "\n",
    "Since we consider the conversion events of depending on offer type differently, we have to first separate the transcript into 3 different offer types, in order to accommodate for the different treatment in assigning the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to split into 3 offer types\n",
    "def split(offer_type,grp_df):\n",
    "    '''\n",
    "    Splits dataframe to groups of specified offer type.\n",
    "    \n",
    "    inputs:\n",
    "    - offer_type: specify offer type name in string format \n",
    "    - grp_df: original transcript dataframe to split on offer type\n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe containing data of just offer type.\n",
    "       \n",
    "    '''\n",
    "    df=grp_df[grp_df['offer_type']==offer_type].copy()\n",
    "    return df\n",
    "\n",
    "#split transcript into 3 different offer types\n",
    "transcript_bogo=split('bogo',transcript)\n",
    "transcript_discount=split('discount',transcript)\n",
    "transcript_info=split('informational',transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each offer type, we can already successfully separate every unique person-offer_id in group 1 from the others using our `valid_completed` column. Since we have flagged all conversion events (`transaction` or `offer completed` event depending on offer type) occurring after an `offer viewed` event, we can be assured that whichever conversion events are flagged with `valid_completed=1` are at least within the first group (People who are influenced and successfully convert - effective offers).\n",
    "\n",
    "For BOGO and discount offers, we will only consider `offer completed` events as the conversion events, while we can consider `transaction` event as the conversion event for the informational offers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since will do this for both BOGO and discount, define function for repeated operation\n",
    "def grp1(df):\n",
    "    '''\n",
    "    Subsets dataframe to just group 1 members.\n",
    "    \n",
    "    inputs:\n",
    "    - df: original transcript dataframe \n",
    "\n",
    "    outputs:\n",
    "    - Returns dataframe containing transcript data of just group 1 users.\n",
    "       \n",
    "    '''\n",
    "    grp1=df[['person','offer_id']][(df['valid_completed']==1) & (df['event']=='offer completed')].groupby(['person','offer_id']).count().reset_index()\n",
    "    return grp1\n",
    "\n",
    "grp1_bogo=grp1(transcript_bogo)\n",
    "grp1_discount=grp1(transcript_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, for informational offers we will define group 1 later as there is an additional consideration we need to take into account for transactions - they need to occur within the validity period of an informational offer for us to consider them as effective offers.\n",
    "\n",
    "Now, we can look into separating group 2 and group 4 unique person-offer_ids for BOGO and discount offers as we just need to look at the subset of people with `offer received`, `offer viewed`, but no conversion events. We can also assume that every person who views an offer would have had an `offer received` event prior, so we can just take the whole group of people who received an offer and subset them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For BOGO offers:\n",
      "            person  offer_id\n",
      "_merge                      \n",
      "left_only        0         0\n",
      "right_only    4729      4729\n",
      "both         20592     20592\n",
      "For Discount offers:\n",
      "            person  offer_id\n",
      "_merge                      \n",
      "left_only        0         0\n",
      "right_only    6415      6415\n",
      "both         18901     18901\n"
     ]
    }
   ],
   "source": [
    "#again, we define a function as we will repeat this for 2 datasets - BOGO & discount\n",
    "def no_conv(df):\n",
    "    \n",
    "    '''\n",
    "    Takes in transcript dataframe of single offer type to check for people who converted vs people with just offer received events. \n",
    "    \n",
    "    inputs:\n",
    "    - df: original transcript dataframe of specific offer type \n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe containing unqiue person-offer_id pairs with conversion events and offers received events, with indicator of each.\n",
    "    \n",
    "    Note: left_only indicator is just the offers received events, right_only is just conversion events\n",
    "       \n",
    "    '''\n",
    "    \n",
    "    #subset offer ids that have transactions or conversions by person and offer_id\n",
    "    conversion_ids=df[['person','offer_id']][(df['event']=='transaction') | (df['event']=='offer completed') ].groupby(['person','offer_id']).count().reset_index()\n",
    "\n",
    "    #check for unique person-offer_id pairs that consist of offers received \n",
    "    offers_received_only=df[['person','offer_id']][df['event']=='offer received'].groupby(['person','offer_id']).count().reset_index()\n",
    "\n",
    "    #create merged dataset to diffrentiate groups\n",
    "    check_merge=conversion_ids.merge(offers_received_only,how='right',on=['person','offer_id'],indicator=True)\n",
    "    return check_merge\n",
    "\n",
    "#check how many are in either group\n",
    "check_merge_bogo=no_conv(transcript_bogo)\n",
    "print('For BOGO offers:')\n",
    "print(check_merge_bogo.groupby(['_merge']).count())\n",
    "\n",
    "check_merge_discount=no_conv(transcript_discount)\n",
    "print('For Discount offers:')\n",
    "print(check_merge_discount.groupby(['_merge']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are definitely a fair number of unique person-offer_id pairs that have `offer received` events, but no conversion events. These would be considered offers in group 2 and 4 within each offer type, according to our definition above. \n",
    "\n",
    "People with an `offer viewed` event in this subset are definitely in group 2, as we can assume everyone with an `offer viewed` event has an `offer received` event prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define group 2 & 4 function as will repeat this for BOGO and discount offers\n",
    "def grp_2_4(df):\n",
    "    \n",
    "    '''\n",
    "    Takes in output dataframe from no_conv function to split into group 2 and 4 customers.\n",
    "    \n",
    "    inputs:\n",
    "    - df: output dataframe from no_conv function\n",
    "    \n",
    "    outputs:\n",
    "    - Returns 2 dataframes containing unique person-offer_id pairs with dataframe containing only group2 customers first, followed by dataframe containing only group 4 customers. \n",
    "       \n",
    "    '''\n",
    "    \n",
    "    #subset to check group 2 and 4\n",
    "    grp_2_4=df[df['_merge']=='right_only']\n",
    "\n",
    "    #remerge with transcript to get events\n",
    "    grp_2_4=grp_2_4.merge(transcript,how='left',on=['person','offer_id'])\n",
    "\n",
    "    #within this subset, separate people with offer viewed event, and people with offer received but no offer viewed\n",
    "    grp2=grp_2_4[['person','offer_id']][grp_2_4['event']=='offer viewed'].groupby(['person','offer_id']).count().reset_index()\n",
    "    \n",
    "    #remerge with full dataset and get remaining to get grp4\n",
    "    drop_cols('_merge',grp_2_4,inplace=True)\n",
    "    grp4=grp_2_4.merge(grp2[['person','offer_id']],how='left',indicator=True)\n",
    "    grp4=grp4[grp4['_merge']=='left_only'].copy()\n",
    "    \n",
    "    return grp2,grp4\n",
    "\n",
    "grp2_bogo,grp4_bogo=grp_2_4(check_merge_bogo)\n",
    "grp2_discount,grp4_discount=grp_2_4(check_merge_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 3 people are everyone in the converted ids who do not have an offer viewed prior - hence, they would be people with conversion events but no `offer viewed` event prior. For BOGO and discount offers, they would be people with `offer completed` events that have `valid_completed != 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grp3(df):\n",
    "    '''\n",
    "    Takes in transcript dataframe of single offer type to check for people who converted vs people with just offer received events. \n",
    "    \n",
    "    inputs:\n",
    "    - df: original transcript dataframe of specific offer type \n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe containing unqiue person-offer_id pairs with conversion events and offers received events, with indicator of each.\n",
    "       \n",
    "    '''\n",
    "    \n",
    "    #check all conversion events with invalid conversions\n",
    "    grp3=df[['person','offer_id']][(df['event']=='offer completed') & (df['valid_completed']!=1)].groupby(['person','offer_id']).count().reset_index()\n",
    "    return grp3\n",
    "\n",
    "grp3_bogo=grp3(transcript_bogo)\n",
    "grp3_discount=grp3(transcript_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have split our data into 4 different customer groups for the BOGO and discount offers. Next, we have to consider the effective and ineffective offers depending on the group type. As already elaborated above, any unique person-offer_id belonging to group 1 can be considered in our target variable `effective_offer=1` group.\n",
    "\n",
    "Meanwhile, group 2 is in our target variable `effective_offer=0` group. For customers in groups 3 and 4, I deprioritise them for model implementation, but will be doing some exploratory analysis on them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offers(grp1,grp2):\n",
    "    '''\n",
    "    inputs:\n",
    "    - grp1: dataframe containing group1 customer data \n",
    "    - grp2: dataframe containing group2 customer data\n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe with labeled effective offer column\n",
    "    '''\n",
    "    #assign effective offer flag column\n",
    "    grp1['effective_offer']=1\n",
    "    grp2['effective_offer']=0\n",
    "\n",
    "    #append datasets together\n",
    "    offers=grp1.append(grp2,sort=False)\n",
    "    return offers\n",
    "\n",
    "offers_bogo=offers(grp1_bogo,grp2_bogo)\n",
    "offers_discount=offers(grp1_discount,grp2_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have successfully prepared the target variables for our BOGO and discount datasets. \n",
    "\n",
    "Meanwhile, for informational offers in particular, before we can tag the effective offers column, there is one more consideration - the validity of the offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Considering duration/validity of offers in converted transactions from informational offers\n",
    "\n",
    "There is an additional rule to consider when considering an effective/converted transaction and offer. This applies for offers that are of type 'informational'. As already elaborated above, the reason why informational offers get a different treatment is because the conversion event is not an `offer completed` event, but a `transaction`.\n",
    "\n",
    "For informational offers, the `duration` of the offer can be considered to be the duration of the influence. Hence, we can make the assumption that an offer should only be considered effective if it is within the `duration` of the offer.\n",
    "\n",
    "Meanwhile, for BOGO and discount offers, we can assume that if there is a conversion/ `offer completed` event, it should be within duration as it would not make sense for an offer to be completed if an offer is past its validity period.\n",
    "\n",
    "As we saw in our data dictionary, the `time` of an event in the `transcript` data is in terms of hours. In order to ensure it is on the same scale as the `duration` of the offer, we have to convert it into days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert time into days\n",
    "transcript_info['day_offer']=transcript_info['time']/24\n",
    "#drop unnecessary columns\n",
    "drop_cols(['time','value','offer_id_previous'],transcript_info,inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort transactions to ensure all events occurring by person and offer\n",
    "transcript_info=transcript_info.sort_values(['person','day_offer','event','offer_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the difference between two events using the `diff` function in pandas. We take the difference between the `transaction` and the `offer received` as the duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get difference in time for informational offers\n",
    "transcript_info['diff_info']=transcript_info[(transcript_info['offer_type']=='informational') & ((transcript_info['event']=='offer received') | (transcript_info['event']=='transaction'))].groupby(['person','offer_id'])['day_offer'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column for flagging valid events\n",
    "transcript_info['valid_completed_duration']=np.nan\n",
    "\n",
    "#flag valid events if within duration\n",
    "transcript_info.loc[transcript_info['diff_info']<=transcript_info['duration'],'valid_completed_duration']=1\n",
    "\n",
    "#fill any missing values with 0 flag\n",
    "transcript_info['valid_completed_duration']=transcript_info['valid_completed_duration'].fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `valid_completed` and `valid_completed_duration` flag columns, we have 4 possible scenarios for an informational offer within the `transcript_info` dataset:\n",
    "\n",
    "|No.| valid_completed | valid_completed_duration | Scenario |\n",
    "|---| --- | --- | --- |\n",
    "|1| 1 | 0 | completed transaction after offer viewed event, but not within duration |\n",
    "|2| 0/null | 1 | completed transaction within duration, but with no offer viewed event prior |\n",
    "|3| 1 | 1 | completed transaction within duration, with offer viewed event - **an effective offer** |\n",
    "|4| 0//null | 0 | did not complete transaction within duration, no offer viewed event prior |\n",
    "\n",
    "Following the above scenarios, only Scenario 3 would be considered our label `effective_offers = 1` for informational offers (group 1 of customers).\n",
    "\n",
    "Meanwhile, Scenarios 1 and 2 can be considered to be actions that would put the customer into our Group 3 of customers - People who purchase/complete offers regardless of awareness of any offers. \n",
    "\n",
    "For customers in Scenario 1, even though according to our `valid_completed` flag, they had viewed an offer prior to the transaction, but it is not within the duration, thus they are not 'influenced' by the offer.\n",
    "\n",
    "Meanwhile for customers in Scenario 2, they are in Group 3 as they completed transactions without viewing an offer. \n",
    "\n",
    "Scenario 4 can be considered in group 4, as they only consist of transactions.\n",
    "\n",
    "We will need to separate those users in group 2 - those who may have received and viewed an offer, but no transactions after. We need to subset those where `effective_offer!=1` into groups 2,3 and 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flag effective_offers where valid_completed=1 and valid_completed_duration=1\n",
    "transcript_info['effective_offer']=np.where(((transcript_info['valid_completed']==1) & (transcript_info['valid_completed_duration']==1)),1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have flagged our effective offers, we can subset them into the 4 groups already outlined above. We can also filter this only for the `effective offers=1` events, as we only want the effective transactions influenced by an offer, not other transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate group 1 in transcript_into\n",
    "grp1_info=transcript_info[['person','offer_id']][transcript_info['effective_offer']==1].groupby(['person','offer_id']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the remaining people, we have to separate it out into groups 2 and 4. We can use similar steps to what we did with BOGO and Discount offers, since we don't have the duration consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For informational offers:\n",
      "            person  offer_id\n",
      "_merge                      \n",
      "left_only        0         0\n",
      "right_only    5412      5412\n",
      "both          7239      7239\n"
     ]
    }
   ],
   "source": [
    "#separate out group 2 of customers\n",
    "check_merge_info=no_conv(transcript_info)\n",
    "print('For informational offers:')\n",
    "print(check_merge_info.groupby(['_merge']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp2_info,grp4_info=grp_2_4(check_merge_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For group 3, we have to consider those with conversions who do not have an offer viewed prior - hence, they would be people with conversion events but no offer viewed event prior. For informational offers, these would be `transaction`s in Scenario 1 and 2 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario 1\n",
    "grp3_1=transcript_info[['person','offer_id']][(transcript_info['event']=='transaction')&(transcript_info['valid_completed']!=1) & (transcript_info['valid_completed_duration']==1)].groupby(['person','offer_id']).count().reset_index()\n",
    "#scenario 2\n",
    "grp3_2=transcript_info[['person','offer_id']][(transcript_info['event']=='transaction')&(transcript_info['valid_completed']==1) & (transcript_info['valid_completed_duration']!=1)].groupby(['person','offer_id']).count().reset_index()\n",
    "grp3_info=grp3_1.append(grp3_2,sort=False)\n",
    "del grp3_1\n",
    "del grp3_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can append the datasets together to make the offers_info dataset, ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_info=offers(grp1_info,grp2_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have subset all our datasets into effective and ineffective offers depending on offer type, we can append the datasets accordingly into datasets for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Feature engineering\n",
    "\n",
    "Now we have to look back had to look into the features and see how to be creative in creating new features.\n",
    "\n",
    "#### d.i. `became_member_on` column to be engineered\n",
    "Recalling my preliminary data exploration steps, the `became_member_on` column were in date format. Hence in order to extract meaningful insights from that feature, we can convert it as a feature indicating tenure of membership. There could be some influence in how long someone has been a member, with whether he takes up an offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column for merging\n",
    "profile.rename(columns={'id':'person'},inplace=True)\n",
    "\n",
    "#create function to reuse for 3 datasets\n",
    "def member(df):\n",
    "    '''\n",
    "    inputs:\n",
    "    - df: original dataframe to transform became_member_on column  \n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe with became_member_on column transformed to be tenure in days\n",
    "    \n",
    "    '''\n",
    "    #merge to get user demographic profile\n",
    "    df=df.merge(profile,how='left',on='person')\n",
    "    \n",
    "    #convert became_member_on into member tenure\n",
    "    df['year']=pd.Series([int(str(x)[:4]) for x in df['became_member_on']])\n",
    "    df['month']=pd.Series([int(str(x)[-3]) for x in df['became_member_on']])\n",
    "    df['day']=pd.Series([int(str(x)[-2:]) for x in df['became_member_on']])\n",
    "    df=drop_cols('became_member_on',df)\n",
    "    df.loc[df['year'] == 2018, 'membership_tenure_days'] = (30*df['month'])+df['day']\n",
    "    df.loc[df['year'] != 2018, 'membership_tenure_days'] = ((2018-df['year'])*365)+(30*df['month'])+df['day']\n",
    "    df=drop_cols(['year','month','day'],df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "offers_bogo=member(offers_bogo)\n",
    "offers_discount=member(offers_discount)\n",
    "offers_info=member(offers_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.ii. Count of offers received\n",
    "As part of some further data exploration, I discovered that there could be multiple offers received per person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n",
      "0009655768c64bdeb2e877511632db8f    5\n",
      "00116118485d4dfda04fdbaba9a87b5c    2\n",
      "0011e0d4e6b944f998e987f904e8c1e5    5\n",
      "0020c2b971eb4e9188eac86d93036a77    5\n",
      "0020ccbbb6d84e358d3414a3ff76cffd    4\n",
      "Name: event, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0bbb118860>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#group event=offer received per person in transactional records\n",
    "print(transcript[transcript['event']=='offer received'].groupby('person')['event'].count().head())\n",
    "\n",
    "#visualise offers received per person\n",
    "transcript[transcript['event']=='offer received'].groupby('person')['event'].count().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the offer received per person in the transactional data could range from 1 to 6 offers received. I had the hypothesis that the frequency of offers received per person might result in more effective offers, so decided to engineer a feature `offer_received_cnt` to account for this frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get count of offers received per person, put into separate dataset\n",
    "df_offer_received_cnt=transcript[transcript['event']=='offer received'].groupby(['person','offer_id','time']).count()['event'].reset_index()\n",
    "\n",
    "#rename columns\n",
    "df_offer_received_cnt.rename(columns={'event':'offer_received_cnt'},inplace=True)\n",
    "\n",
    "#drop unnecessary columns\n",
    "drop_cols('time',df_offer_received_cnt,inplace=True)\n",
    "\n",
    "#ensure only unique person-offer_id pairs\n",
    "df_offer_received_cnt=df_offer_received_cnt.groupby(['person','offer_id']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.ii. Separating user behaviours by transactions\n",
    "\n",
    "I also wondered how many transactions were considered 'invalid' by my definition. Ordinarily, these would be the sum of transactions done by people not in group 1. The objective of offers are to drive purchases, so it would already be the case that users with high spend in their transactions would be flagged as `effective_offers`. \n",
    "\n",
    "We've already defined that there are people in groups 3 and 4, where they are separate pools of users who are loyal spenders, and already tend to purchase more, isolated from the the effect of offers. \n",
    "\n",
    "But for users in group 1 have a high amount of 'invalid spend' outside of the effect of offers, there might be some predictive power onto the effectiveness of offers; since a loyal user might have a higher tendency of taking up an offer.\n",
    "\n",
    "In my datasets, I had already separated the transactions who are conversions versus transactions who are just the users' normal purchasing behaviour. This is through the `valid_completed` column, where I checked if a transaction had an `offer viewed` event prior. \n",
    "\n",
    "In the cases where `valid_completed`=1, I had already included them in my effective offers flag for BOGO and Discount offers. However, for those transctions where `valid_completed`=0, I have not considered them, and this could be a potential feature to include, as a proxy for the 'baseline' level of spending for a user.\n",
    "\n",
    "The logic is to wonder if there is some baseline level of spending for users who are highly influenced by certain offers (in group 1), and group 2, and if there is some predictive power in this baseline level of 'invalid transactions' that can predict the propensity of a user to take up an offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter dataset by invalid transactions\n",
    "df_transactions_invalid=transcript[(transcript['event']=='transaction') & (transcript['valid_completed']==0)].groupby(['person','offer_id'])['amount'].sum().reset_index()\n",
    "df_transactions_invalid.rename(columns={'amount':'amount_invalid'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. iii. Time elapsed between offers received\n",
    "\n",
    "I also wanted to include time as a potential feature into my dataset, but since the transactional data starts from time=0, I suspected it would not have been of much predictive power without some feature engineering. I had the hypothesis that if there were multiple offers received per person within a certain time period, there might be some predictive power in the time elapsed between offers received. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    706\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[0;32m--> 707\u001b[0;31m                                                    self.axis)\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mcurried_with_axis\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mcurried_with_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_with_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: diff() got an unexpected keyword argument 'axis'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurried_with_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0m_group_selection_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    706\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[0;32m--> 707\u001b[0;31m                                                    self.axis)\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mcurried_with_axis\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mcurried_with_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_with_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: diff() got an unexpected keyword argument 'axis'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-024066f6dd2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#find time elapsed between offers received\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtranscript\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_elapsed_offers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'event'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'offer received'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'person'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'offer_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day_offer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#fill missing values with 0, as if someone does not receive an offer or is receiving an offer for the first time, there is no time elapsed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m                       examples=_apply_docs['series_examples']))\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     @Substitution(see_also=_agg_see_also_doc,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode.chained_assignment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         keys, values, mutated = self.grouper.apply(f, self._selected_obj,\n\u001b[0;32m--> 707\u001b[0;31m                                                    self.axis)\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mresult_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;31m#     raise AssertionError('Start %s must be less than end %s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;31m#                          % (str(start), str(end)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_sorted_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36m_chop\u001b[0;34m(self, sdata, slice_obj)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_chop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_values\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             return self._constructor(self._data.get_slice(indexer),\n\u001b[0m\u001b[1;32m    978\u001b[0m                                      fastpath=True).__finalize__(self)\n\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mget_slice\u001b[0;34m(self, slobj, axis)\u001b[0m\n\u001b[1;32m   1509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         return self.__class__(self._block._slice(slobj),\n\u001b[0;32m-> 1511\u001b[0;31m                               self.index[slobj], fastpath=True)\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3961\u001b[0m             \u001b[0;31m# This case is separated from the conditional above to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3962\u001b[0m             \u001b[0;31m# pessimization of basic indexing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3963\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpromote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/numeric.py\u001b[0m in \u001b[0;36m_shallow_copy\u001b[0;34m(self, values, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_na\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# Ensure we are not returning an Int64Index with float data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shallow_copy_with_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         return (super(NumericIndex, self)._shallow_copy(values=values,\n\u001b[1;32m     73\u001b[0m                                                         **kwargs))\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_shallow_copy_with_infer\u001b[0;34m(self, values, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, fastpath, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m                 PeriodIndex, IncompatibleFrequency)\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mis_signed_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumeric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInt64Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mInt64Index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_signed_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m     return _is_dtype_type(\n\u001b[0;32m--> 977\u001b[0;31m         arr_or_dtype, classes_and_not_datetimelike(np.signedinteger))\n\u001b[0m\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36m_is_dtype_type\u001b[0;34m(arr_or_dtype, condition)\u001b[0m\n\u001b[1;32m   1863\u001b[0m     \u001b[0;31m# fastpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1865\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1866\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPandasExtensionDtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(tipo)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdatetimelike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \"\"\"\n\u001b[0;32m--> 127\u001b[0;31m     return lambda tipo: (issubclass(tipo, klasses) and\n\u001b[0m\u001b[1;32m    128\u001b[0m                          not issubclass(tipo, (np.datetime64, np.timedelta64)))\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#convert time into days\n",
    "transcript['day_offer']=transcript['time']/24\n",
    "#drop unnecessary columns\n",
    "drop_cols(['time'],transcript,inplace=True);\n",
    "\n",
    "#find time elapsed between offers received\n",
    "transcript['time_elapsed_offers']=transcript[transcript['event']=='offer received'].groupby(['person','offer_id'])['day_offer'].diff()\n",
    "\n",
    "#fill missing values with 0, as if someone does not receive an offer or is receiving an offer for the first time, there is no time elapsed\n",
    "transcript['time_elapsed_offers']=transcript['time_elapsed_offers'].fillna(value=0)\n",
    "\n",
    "#create temporary dataset\n",
    "df_time_elapsed=transcript.groupby(['person','offer_id'])['time_elapsed_offers'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Preparing data for implementation\n",
    "\n",
    "Now we can finally begin with preparing the data for modeling. \n",
    "\n",
    "To do this, there are some additional preparation steps for each dataset. Recalling our initial preliminary data exploration, there are some steps to prepare the data:\n",
    "\n",
    "a. Merge with temporary datasets created above to include engineered features\n",
    "\n",
    "b. Drop missing values in `gender` column for demographic data; convert gender into dummy variables\n",
    "\n",
    "c. Separate the `channel` column into categorical variables\n",
    "\n",
    "d. Treatment of duplicate records\n",
    "\n",
    "#### f. ii.a. Merge with temporary datasets created above to include engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge to get offers received count and invalid amount transacted \n",
    "offers_bogo=offers_bogo.merge(df_offer_received_cnt[['person','offer_id','offer_received_cnt']],how='left',on=['person','offer_id'])\n",
    "offers_bogo=offers_bogo.merge(df_transactions_invalid[['person','offer_id','amount_invalid']],how='left',on=['person','offer_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f.ii.b. Drop missing values in gender column for demographic data\n",
    "\n",
    "Now, we need to check whether dropping the missing values will result in a significant loss in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check % of missing values in dataset\n",
    "(offers_bogo.isnull().sum()/len(offers_bogo)*100).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the missing values are quite extensive especially for the `amount_invalid` column. It is debatable whether this column `amount_invalid` would be useful to include in the model. Since it is so 'sparse' for BOGO offers, it might not have much information after all. I plan to assess this feature again later during the model implementation phase. For now, I decided to fill the missing `amount_invalid` column with 0 as it could represent that only 3% of the overall users tend to purchase without offers; the other 97% would only purchase with awareness of an ongoing offer. \n",
    "\n",
    "Meanwhile, we had already conducted the analysis above on the `income` and  `gender` columns, which I choose to drop as they are not useful when they are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing values for amount_invalid with 0\n",
    "offers_bogo['amount_invalid']=offers_bogo['amount_invalid'].fillna(value=0)\n",
    "\n",
    "#drop income and gender null rows\n",
    "offers_bogo.dropna(inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f.ii.c. Separate the channel column into categorical variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foresee need to reuse function so create rename function\n",
    "def rename(col_name,df):\n",
    "    df[col_name]=np.where(df[col_name]==col_name,1,0)\n",
    "    return df\n",
    "\n",
    "#foresee need to reuse dummy variable encoding function\n",
    "def dummy(df,col):\n",
    "    df=pd.concat([df[:],pd.get_dummies(df[col],prefix=col)],axis=1)\n",
    "    df=drop_cols(col,df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with portfolio to get offer details\n",
    "offers_bogo=offers_bogo.merge(portfolio,how='left',on='offer_id')\n",
    "\n",
    "#convert channels into categorical variables\n",
    "channels = offers_bogo['channels'].apply(pd.Series)\n",
    "channels = channels.rename(columns={0:'web',1:'email',2:'mobile',3:'social'})\n",
    "offers_bogo=pd.concat([offers_bogo[:], channels[:]], axis=1)\n",
    "rename('web',offers_bogo)\n",
    "rename('email',offers_bogo)\n",
    "rename('mobile',offers_bogo)\n",
    "rename('social',offers_bogo)\n",
    "offers_bogo=drop_cols('channels',offers_bogo)\n",
    "\n",
    "#convert gender into categorical variables\n",
    "offers_bogo=dummy(offers_bogo,'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to repeat these steps for `offers_discount`, I created a function containing all the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_offers_df(df):\n",
    "    \n",
    "    '''\n",
    "    inputs:\n",
    "    - df: original dataframe for modeling \n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe containing engineered features, filled missing values and cleaned and transformed variables (channel and gender)\n",
    "       \n",
    "    '''\n",
    "    #merge to get engineered features \n",
    "    df=df.merge(df_offer_received_cnt[['person','offer_id','offer_received_cnt']],how='left',on=['person','offer_id'])\n",
    "    df=df.merge(df_transactions_invalid[['person','offer_id','amount_invalid']],how='left',on=['person','offer_id'])\n",
    "    \n",
    "    #fill missing values for amount_invalid with 0\n",
    "    df['amount_invalid']=df['amount_invalid'].fillna(value=0)\n",
    "    \n",
    "    #drop income and gender null rows\n",
    "    df.dropna(inplace=True);\n",
    "    \n",
    "    #merge with portfolio to get offer details\n",
    "    df=df.merge(portfolio,how='left',on='offer_id')\n",
    "\n",
    "    #convert channels into categorical variables\n",
    "    channels = df['channels'].apply(pd.Series)\n",
    "    channels = channels.rename(columns={0:'web',1:'email',2:'mobile',3:'social'})\n",
    "    df=pd.concat([df[:], channels[:]], axis=1)\n",
    "    rename('web',df)\n",
    "    rename('email',df)\n",
    "    rename('mobile',df)\n",
    "    rename('social',df)\n",
    "    df=drop_cols('channels',df)\n",
    "    \n",
    "    #convert gender column into dummy variables\n",
    "    df=dummy(df,'gender')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for offer_discounts\n",
    "offers_discount=prep_offers_df(offers_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `offers_info` dataset, a slightly different treatment needs to be done as the `channels` column contains a different order of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with portfolio to get offer details\n",
    "offers_info=offers_info.merge(portfolio,how='left',on='offer_id')\n",
    "\n",
    "#reset index for offers_info\n",
    "offers_info=drop_cols('index',offers_info.reset_index())\n",
    "\n",
    "#expand channel column into categorical variables\n",
    "def channel_col(name,df=offers_info):\n",
    "    '''\n",
    "    inputs:\n",
    "    - name: name of channel column to be transformed \n",
    "    - df: dataframe \n",
    "    \n",
    "    outputs:\n",
    "    - offer_info dataframe with channel column transformed\n",
    "    \n",
    "    '''\n",
    "    df[name]= np.nan\n",
    "    df.loc[pd.Series([name in df['channels'][x] for x in range(len(df['channels']))]),name]=1\n",
    "    df[name]=df[name].fillna(value=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_col('web')\n",
    "channel_col('email')\n",
    "channel_col('mobile')\n",
    "channel_col('social');\n",
    "\n",
    "drop_cols('channels',offers_info,inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repurpose function for offers_info\n",
    "def prep_offers_df(df):\n",
    "    '''\n",
    "    inputs:\n",
    "    - df: dataframe to be transformed \n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe with engineered features and filled missing values, with transformed gender column.\n",
    "       \n",
    "    '''\n",
    "    #merge to get engineered features \n",
    "    df=df.merge(df_offer_received_cnt[['person','offer_id','offer_received_cnt']],how='left',on=['person','offer_id'])\n",
    "    df=df.merge(df_transactions_invalid[['person','offer_id','amount_invalid']],how='left',on=['person','offer_id'])\n",
    "\n",
    "    #fill missing values for amount_invalid and offer_received_cnt with 0\n",
    "    df['amount_invalid']=df['amount_invalid'].fillna(value=0)\n",
    "\n",
    "    #drop income and gender null rows\n",
    "    df.dropna(inplace=True);\n",
    "    \n",
    "    #convert gender column into dummy variables\n",
    "    df=dummy(df,'gender')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_info=prep_offers_df(offers_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "offers_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f.i. Treatment of duplicate records\n",
    "\n",
    "Since we have subset the data cleanly according to unique person-offer_id pairs by group, we should not have any duplicate records. But just in case, we check to make sure we have no duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check multiple records for each person and offer ids for the target variable\n",
    "print((offers_bogo.groupby(['person','offer_id','effective_offer']).size()>1).sum())\n",
    "print((offers_discount.groupby(['person','offer_id','effective_offer']).size()>1).sum())\n",
    "print((offers_info.groupby(['person','offer_id','effective_offer']).size()>1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Now that the datasets are ready, we can proceed to implementing the model. Revisiting our objective, we wanted to analyse the drivers of an effective offer, with the target variable being `effective_offer`.\n",
    "\n",
    "Since we have 3 offer types, there are thus 3 different models to be built. Since we are predicting whether an offer would be effective or not, this is effectively a binary classification supervised learning model.\n",
    "\n",
    "I decided to compare the performance of a simple decision tree classifier model as a baseline model, with an ensemble random forest classifier model. Reason why I selected a decision tree as the baseline model is because I wanted to prioritise the interpretability of the model. Going back to the objective, since we intend to analyse the feature importance to determine the drivers of an effective offer, a decision tree would provide good interpretability for us to analyse.\n",
    "\n",
    "Meanwhile, I also selected random forest as an alternate model to compare the baseline model is as an improvement over simple ensemble bagging of decision trees, in order to drive towards a high accuracy in training the model. \n",
    "\n",
    "Before we can proceed, we have to make sure that the classes we are predicting for are balanced in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for class balance in datasets\n",
    "print(offers_bogo[['person','effective_offer']].groupby('effective_offer').count()/len(offers_bogo))\n",
    "print(offers_discount[['person','effective_offer']].groupby('effective_offer').count()/len(offers_discount))\n",
    "print((offers_info[['person','effective_offer']].groupby('effective_offer').count()/len(offers_info)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the classes are quite uneven for all three offer types, but not too imbalanced such that it would pose a problem. Hence, we can proceed to implement the models.\n",
    "\n",
    "A note on model evaluation and validation; since the classes for the all 3 models are imbalanced, I decided to implement both accuracy and f1 score as the model evaluation metric. F1 score provides a better sense of model performance compared to purely accuracy as takes both false positives and false negatives in the calculation. With an uneven class distribution, F1 may usually be more useful than accuracy. \n",
    "\n",
    "It is worth noting in this case that the F1 score is based on the harmonic mean of precision and recall, and focuses on positive cases. For the Starbucks app here, it would be fine as we would prioritise more on whether offers are effective, and less focus on why offers are ineffective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Model Implementation\n",
    "\n",
    "Revisiting our objective, we are creating 3 models to predict the effectiveness of an offer within each type, depending on offer attributes and user demographics.\n",
    "\n",
    "First, we have to define our target and features variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df,drop_cols_prep):\n",
    "    '''\n",
    "    inputs:\n",
    "    - df: prepared dataframe for modeling \n",
    "    \n",
    "    outputs:\n",
    "    - Returns 2 dataframes - features and target dataframes\n",
    "    '''\n",
    "    # Split the data into features and target label\n",
    "    target = df['effective_offer']\n",
    "    features = drop_cols(drop_cols_prep,df)\n",
    "    return features,target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I split the data into training and test sets. Since the features of my data are all on different scales, I also apply a scaler to ensure my data will all be on the same scale for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare model pipeline\n",
    "def model_pipeline(features,target):\n",
    "    '''\n",
    "    inputs:\n",
    "    - features & target dataframe \n",
    "    \n",
    "    outputs:\n",
    "    - Splits features and target dataframe to train and test sets, performs feature scaling on both datasets.\n",
    "    - Outputs X_train, X_test, y_train and y_test dataframes\n",
    "    '''\n",
    "    \n",
    "    #split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features,target, \n",
    "                                                        test_size=0.20, \n",
    "                                                        random_state=42)\n",
    "\n",
    "    #fit and transform scaling on training data\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "\n",
    "    #scale test data\n",
    "    X_test=scaler.transform(X_test)\n",
    "    return X_train,X_test,y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am defining the functions here to run my model as I plan to implement 3 different models; hence it would be easier to implement repeatedly. In this function, I define the model scores - F1 score and accuracy, as well as the error (mean squared error). As elaborated above, I plan to compare the F1 score with the accuracy score as a better indication of model performance, especially since the classes for the BOGO and discount offers are uneven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(learner, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: review_scores_rating training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: review_scores_rating testing set\n",
    "    '''\n",
    "    results = {}\n",
    "    \n",
    "    #Fit the learner to the training data and get training time\n",
    "    start = time() \n",
    "    learner = learner.fit(X_train, y_train)\n",
    "    end = time() \n",
    "    results['train_time'] = end-start\n",
    "    \n",
    "    # Get predictions on the test set(X_test), then get predictions on first 300 training samples\n",
    "    start = time() \n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train)\n",
    "    end = time() \n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end-start\n",
    "    \n",
    "    #add training accuracy to results\n",
    "    results['training_score']=learner.score(X_train,y_train)\n",
    "    \n",
    "    #add testing accuracy to results\n",
    "    results['testing_score']=learner.score(X_test,y_test)\n",
    "     \n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, len(y_train)))\n",
    "    print(\"MSE_train: %.4f\" % mean_squared_error(y_train,predictions_train))\n",
    "    print(\"MSE_test: %.4f\" % mean_squared_error(y_test,predictions_test))\n",
    "    print(\"Training accuracy:%.4f\" % results['training_score'])\n",
    "    print(\"Test accuracy:%.4f\" % results['testing_score'])\n",
    "    print(classification_report(y_test, predictions_test,digits=4))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(clf1,clf2,name):\n",
    "    '''\n",
    "    inputs:\n",
    "    - clf1: first classifier model\n",
    "    - clf2: 2nd classifier model for comparison\n",
    "    - name: name of models for comparison\n",
    "    \n",
    "    outputs:\n",
    "    - Dataframe of results from model training and prediction\n",
    "    '''\n",
    "    \n",
    "    # Collect results on the learners\n",
    "    results = {}\n",
    "    for clf in [clf1, clf2]:\n",
    "        clf_name = clf.__class__.__name__ + '_' +name\n",
    "        results[clf_name] = {}\n",
    "        results[clf_name]= train_predict(clf, X_train, y_train, X_test, y_test)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.i. BOGO offers model\n",
    "\n",
    "First we try to build the BOGO offers model. I initialize the models with some randomly chosen parameters to check the initial performance. If performance needs to be improved further, I will attempt Grid Search to find the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type']\n",
    "features,target=data_prep(offers_bogo,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model - baseline is DT model, bogo_1 model is RF model\n",
    "baseline = DecisionTreeClassifier(criterion='entropy',max_depth=5,random_state=2,min_samples_split=90,min_samples_leaf=50)\n",
    "bogo_1 = RandomForestClassifier(random_state=2,max_depth= 11, max_features= 'auto',min_samples_split= 10,n_estimators=20,min_samples_leaf=20)\n",
    "\n",
    "results=run_model(baseline,bogo_1,'bogo_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for Random Forest Classifier (RF) model actually ends up outperforming the Decision Tree Classifier (DT) model slightly, but overall the performance for both models is about the same (82.14% vs 81.77% respectively in terms of accuracy). \n",
    "Accuracy for a first attempt is quite good, more than 80%. I will try to tune the model further to get a better accuracy.\n",
    "\n",
    "However, in terms of the F1 score, both models are below 80%, with the Random Forest model performing worse compared to the Decision Tree Classifier, with 75.91% vs. 79.63%. To analyse this, we have to refer to the formula for Precision, Recall and F1 score:\n",
    "\n",
    "**Recall or Sensitivity or TPR (True Positive Rate):** \n",
    "\n",
    "According to sklearn documentation, the recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "Number of items correctly identified as positive out of total true positives: True Positives /(True Positives +False Negatives)\n",
    "\n",
    "**Precision:** \n",
    "\n",
    "According to the sklearn documentation, it is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "Number of items correctly identified as positive out of total items identified as positive: True Positives /(True Positives + False Positives)\n",
    "\n",
    "\n",
    "**F1 Score:** \n",
    "\n",
    "Since my F-beta score is F1 with beta=1, I am weighting recall and precision as equally important.\n",
    "\n",
    "The formula is given by the harmonic mean of precision and recall:  F1 = 2*Precision*Recall/(Precision + Recall)\n",
    "\n",
    "We can see that the F1 scores for DT outperformed RF slightly, but both are lower than the accuracy. This would indicate that DT model is doing slightly better compared to RF at not misclassifying negative events as positive (meaning, misclassifying people on which offers are ineffective, as people on which offers would be effective).  \n",
    "\n",
    "The difference in F1 score vs accuracy indicate that there could are instances where both models are falsely classifying negatives as positives, likely due to the imbalance of classes. But the overall higher recall/accuracy compared to F1 score indicates that the model is predicting the positive case (i.e. where an offer is effective) more accurately compared to predicting the negative cases (i.e. where an offer is ineffective), which is expected given the uneven classes..\n",
    "\n",
    "However, revisiting our use case, we are perhaps not as concerned with these misclassification since we don't mind sending people more offers than they would have liked; we would rather not miss anyone on which an offer would have been effective.\n",
    "\n",
    "Given this case, I will still go with the RF model.\n",
    "\n",
    "Since I aim to analyse the drivers of an effective offer, I will check the feature importances for the models after I have selected the best model from refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.ii. Discount offers model\n",
    "\n",
    "I repeat the same steps above but with my offer_discounts dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type']\n",
    "features,target=data_prep(offers_discount,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "discount_1 = RandomForestClassifier(random_state=2,max_depth= 20, max_features= 'auto',min_samples_split= 10,n_estimators=20,min_samples_leaf=10)\n",
    "results=pd.concat([results[:],run_model(baseline,discount_1,'discount_1')],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the Random Forest Classifier model also has a better performance compared to the Decision Tree Classifier in terms of accuracy (87.23% vs 86.72%), and the F1 score is also lower (81.43% vs 82.87%). \n",
    "\n",
    "The F1 score for these models are lower overall compared to the Accuracy score. This could be an indication that there are some instances where both models are classifying the negative cases (effective_offer = 0) falsely. Again, I am not too bothered by this as I am more concerned with the model predicting positive cases accurately, so would rather go with a higher accuracy model where F1 score for cases `effective_offer=1` is higher, for which our RF classifier has better performance (0.9317 vs 0.9280)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.iii.  Informational offers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,target=data_prep(offers_info,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "info_1 = RandomForestClassifier(random_state=5,criterion='gini',max_depth= 20, max_features= 'auto',min_samples_split= 10,n_estimators=20,min_samples_leaf=10)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,info_1,'info_1')],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance for these models are worse compared to the other 2 datasets, with accuracy below 80% for both models, but RF model still performing better. The F1 score is also worse, at 67.54% RF Classifier, worse than the DT model at 68.66%.\n",
    "\n",
    "One potential reason for the worse performance is perhaps due to the fact that I had the key assumption to assign the conversion events to be transactions that only occur after an offer is viewed and within the specified duration; I might have missed out on some valuable information by removing those transactions that occur regardless. We can see this from how the overall sample dataset is smaller (about half) the datasets for the other 2 offers, with only about 5K samples compared to about 10K for both BOGO and discount respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Refinement\n",
    "\n",
    "In refining the model, I will first try parameter tuning for the 3 RF models, before expreimenting with removing or adding features to improve model performance. \n",
    "\n",
    "Since I will be comparing the models based on testing score repeatedly, I built a function to find the best RF model results based on refinement depending on offer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to find best model results for each offer type\n",
    "def best_model(offer_type):\n",
    "    '''\n",
    "    input:\n",
    "    - offer_type: string of offer type name\n",
    "    output:\n",
    "    - dataframe containing results of best model so far\n",
    "    \n",
    "    '''\n",
    "    print('For ' + offer_type + ' RF model:')\n",
    "    return results.transpose()[results.transpose()['testing_score']==results.transpose()[results.transpose().index.str.contains(\"RandomForestClassifier_\"+offer_type)]['testing_score'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. i. Grid Search to discover optimal parameters\n",
    "I decided to do GridSearch to determine what would be the optimal parameters for the model.\n",
    "\n",
    "For all three offers, the Random Forest model had relatively good performance, so I used Grid Search on this to determine the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Grid Search function\n",
    "def rand_forest_param_selection(X,y):\n",
    "    '''\n",
    "    input:\n",
    "    - X,y: training datasets for X and y\n",
    "    output:\n",
    "    - dictionary with best parameters for random forest model\n",
    "    '''\n",
    "    \n",
    "    param_grid={'max_features': ['auto', 'sqrt'],\n",
    "                'max_depth' : [5,10,15,20],\n",
    "                'n_estimators': [10,20,25,30,40,50],\n",
    "                'min_samples_split': [2, 10, 20],\n",
    "                'min_samples_leaf': [2, 10,15, 20],\n",
    "                }\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(random_state=2), param_grid)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BOGO dataset\n",
    "features,target=data_prep(offers_bogo,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#run Grid Search - commented out because takes to long to run, but have put in selected params in model\n",
    "# rand_forest_param_selection(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the optimal parameters for the BOGO model, I run my model again with the new parameters, keeping the DecisionTree baseline model with the same parameters as comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,target=data_prep(offers_bogo,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "bogo_2 = RandomForestClassifier(random_state=2,max_depth= 10, max_features= 'auto',min_samples_split= 20,n_estimators=30,min_samples_leaf=2)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,bogo_2,'bogo_2')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_bogo_1','RandomForestClassifier_bogo_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for BOGO offer type\n",
    "best_model('bogo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for the RF model increased slightly - from 82.14% to 82.51%, and the F1 score increased from 75.91% to 77.64%. This is a good performance increase but minimal, which indicates that perhaps there's not much that can be done to improve the performance of the model with parameter tuning. \n",
    "\n",
    "So I will have to explore other avenues with the features to improve the performance of the model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define discount dataset\n",
    "features,target=data_prep(offers_discount,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "# run Grid Search - commented out because takes to long to run, but have put in selected params in model\n",
    "# rand_forest_param_selection(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "discount_2 = RandomForestClassifier(random_state=2,max_depth= 10, max_features= 'auto',min_samples_split= 20,n_estimators=30,min_samples_leaf=2)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,discount_2,'discount_2')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_discount_1','RandomForestClassifier_discount_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for discount offer type\n",
    "best_model('discount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model increaased slightly, from 87.23% to 87.47%, and the F1 score improved from 81.43% to 82.06%. The good thing is that now both the accuracy and the F1 score for the RF model is better than the DT model. \n",
    "\n",
    "But because the increase was minimal, again we can conclude that tuning the parameters won't really improve the performance of the model significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define info dataset\n",
    "features,target=data_prep(offers_info,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#run Grid Search - commented out because takes to long to run, but have put in selected params in model\n",
    "# rand_forest_param_selection(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features,target=data_prep(offers_info,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "info_2 = RandomForestClassifier(random_state=2,max_depth= 15, max_features= 'auto',min_samples_split= 2,n_estimators=20,min_samples_leaf=15)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,info_2,'info_2')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_info_1','RandomForestClassifier_info_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for info offer type\n",
    "best_model('info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see some improvement in accuracy for RF model, from 75.09% to 75.30%, and slight increase in F1 score from 67.54% to 67.78%. This improvement is minimal,so we look into improving the feature selection of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.ii Removing sparse features e.g. amount_invalid\n",
    "\n",
    "In terms of feature selection, I wanted to try and see if removing the amount_invalid variable, which we had noted as being sparse, hence may not be useful in predicting the effectiveness of offers, would help.\n",
    "\n",
    "I removed the feature from my data prep and retrained the model using the same optimal parameters found via GridSearch, with the DT model as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add amount_invalid variable to drop_cols_prep list\n",
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type','amount_invalid']\n",
    "\n",
    "#train BOGO model\n",
    "features,target=data_prep(offers_bogo,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "bogo_3 = RandomForestClassifier(random_state=2,max_depth= 10, max_features= 'auto',min_samples_split= 20,n_estimators=30,min_samples_leaf=2)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,bogo_3,'bogo_3')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_bogo_2','RandomForestClassifier_bogo_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for BOGO offer type\n",
    "best_model('bogo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model accuracy and F1 score did improve, so I will leave the amount_invalid feature out of my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train discount model\n",
    "features,target=data_prep(offers_discount,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "discount_3 = RandomForestClassifier(random_state=2,max_depth= 10, max_features= 'auto',min_samples_split= 20,n_estimators=30,min_samples_leaf=2)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,discount_3,'discount_3')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_discount_2','RandomForestClassifier_discount_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for discount offer type\n",
    "best_model('discount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the model actually increased while F1 model remained the same. In this case, I will also remove the amount_invalid feature for the discount model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train info model\n",
    "features,target=data_prep(offers_info,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "info_3 = RandomForestClassifier(random_state=2,max_depth= 15, max_features= 'auto',min_samples_split= 2,n_estimators=20,min_samples_leaf=15)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,info_3,'info_3')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_info_2','RandomForestClassifier_info_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for info offer type\n",
    "best_model('info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and F1 score of the model actually decreased here for info model, so I will also keep the feature in. This is expected since the model had already a worse performance compared to the other 2 models, so the model is slightly underfitting compared to the others. Hence the model needs more features to learn to predict better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. iii. Dropping one level of dummy variables/one-hot encoding\n",
    "\n",
    "There is a debate when using tree models and using regression models when it comes to one hot encoding. For regression classification models (e.g. logistic regression, we should typically remove one level of the variable in order to prevent multicollinearity between variables. Typically, we should not run into this issue with tree-based models like the ones I am using here. \n",
    "\n",
    "However, there is some debate as to whether one should do it or not. According to some articles (like here: https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/), it is generally not advisable to encode categorical variables as they would generate sparse matrices, resulting in:\n",
    "\n",
    "1. The resulting sparsity virtually ensures that continuous variables are assigned higher feature importance.\n",
    "2. A single level of a categorical variable must meet a very high bar in order to be selected for splitting early in the tree building. This can degrade predictive performance.\n",
    "\n",
    "In scikitlearn implementations of RF and DT, one has to encode the variables. So I decided to test my model performance if I were to drop one level of my categorical variables (in my data - the channel variables and the gender variables), just to reduce the sparsity and noise in the data for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add one level of dummy variables to drop column \n",
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type','amount_invalid','social','gender_O']\n",
    "features,target=data_prep(offers_bogo,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model - reuse best performing model - \n",
    "bogo_4 = RandomForestClassifier(random_state=2,max_depth= 10, max_features= 'auto',min_samples_split= 20,n_estimators=30,min_samples_leaf=2)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,bogo_4,'bogo_4')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_bogo_3','RandomForestClassifier_bogo_4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for BOGO offer type\n",
    "best_model('bogo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of this model was not as good as previous model - hence I will keep alll levels of variables in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,target=data_prep(offers_discount,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model - reuse best performing model - \n",
    "discount_4 = RandomForestClassifier(random_state=2,max_depth= 10, max_features= 'auto',min_samples_split= 20,n_estimators=30,min_samples_leaf=2)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,discount_4,'discount_4')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_discount_3','RandomForestClassifier_discount_4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for discount offer type\n",
    "best_model('discount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, accuracy of model was not as good, and minimal improvement. Hence I will keep all levels in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,target=data_prep(offers_info,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model - reuse best performing model - \n",
    "info_4 = RandomForestClassifier(random_state=2,max_depth= 15, max_features= 'auto',min_samples_split= 2,n_estimators=20,min_samples_leaf=15)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,info_4,'info_4')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_info_3','RandomForestClassifier_info_4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for info offer type\n",
    "best_model('info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we have seen that there is not much improvement in model performance just by reducing one level of categorical features. I am quite satisfied with the performance of the BOGO and discount models, but want to explore if I can improve the performance of the info model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. iv. Using polynomial features\n",
    "\n",
    "Since a low accuracy score for the info model is likely due to the model underfitting, I decided to attempt if transforming the features further might improve model performance.\n",
    "\n",
    "I tweaked my model_pipeline function to include the polynomial features transformation to my features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare model pipeline\n",
    "def model_pipeline_poly(features,target,poly_feat=0):\n",
    "    '''\n",
    "    input:\n",
    "    - features & target dataframes\n",
    "    - poly_feat: number of degrees to transform polynomial features\n",
    "    \n",
    "    output:\n",
    "    - X_train, X_test, y_train, y_test dataframes\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features,target, \n",
    "                                                        test_size=0.20, \n",
    "                                                        random_state=42)\n",
    "    #fit and transform training data\n",
    "    poly = PolynomialFeatures(poly_feat)\n",
    "    X_train_poly=poly.fit_transform(X_train)\n",
    "    \n",
    "    #transform test data\n",
    "    X_test_poly=poly.transform(X_test)\n",
    "    \n",
    "    #fit and transform scaling on training data\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train_poly)\n",
    "\n",
    "    #scale test data\n",
    "    X_test=scaler.transform(X_test_poly)\n",
    "    return X_train,X_test,y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep amount_invalid in offers_info dataset\n",
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type']\n",
    "features,target=data_prep(offers_info,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline_poly(features,target,2)\n",
    "\n",
    "#Initialize the model\n",
    "info_5 = RandomForestClassifier(random_state=2,max_depth= 15, max_features= 'auto',min_samples_split= 2,n_estimators=20,min_samples_leaf=15)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,info_5,'info_5')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_info_2','RandomForestClassifier_info_5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for info offer type\n",
    "best_model('info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that performance actually decreased slightly for the RF model. Hence it would perhaps be a better idea to just keep the model as is. A maximum accuracy of 75.30% is acceptable for the info offers, even though it is not as high as the BOGO or discount offers. After all, we already included some assumptions for the 'influence' of the offer based on the duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[['training_score','testing_score'],['RandomForestClassifier_info_1','RandomForestClassifier_info_2','RandomForestClassifier_info_3','RandomForestClassifier_info_4','RandomForestClassifier_info_5']].transpose().plot.line()\n",
    "plt.title('Training and Testing score for RF info models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note however, we can above actually see the model is performing better in the training accuracy as we add more variables for each model via polynomial features and removing the amount_invalid feature. It is just that the testing accuracy was reducing, and we can see this is due to overfitting.\n",
    "\n",
    "I can improve the accuracy and performance of the info model further by using RF info model 5, but adding more data, as we already noted the dataset for the `offers_info` dataset is half the size of the BOGO and discount datasets. Hence, ultimately with more data and with performance tuning, removing unnecessary variables and feature transformation, with more data I could have ultimately got the performance of the model perhaps above 80%.\n",
    "\n",
    "**b.iv. Discussion on best models and feature importances:**\n",
    "\n",
    "Now that I am done with refining the 3 models, we can check the results for our best models for all 3 and check the feature importances to see the top drivers of effectiveness of offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best model overall for bogo,discount and info offers\n",
    "best_model('bogo').append([best_model('discount'),best_model('info')]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we can see that the top performing models are the 3rd model (with GridSearch to find optimal model parameters and removing amount_invalid column) for predicting effectiveness of BOGO and discount offers, whereas the best performing model for informational offers was just after performing GridSearch to find the optimal parameters.\n",
    "\n",
    "In order to find the most influential drivers of an effective offer, we can check the feature importances of our best models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show feature importance\n",
    "#BOGO 3 model\n",
    "#prepare data same as BOGO 3 state\n",
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type','amount_invalid']\n",
    "features,target=data_prep(offers_bogo,drop_cols_prep)\n",
    "\n",
    "feature_importances = pd.DataFrame(bogo_3.feature_importances_,\n",
    "                                   index = features.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "feature_importances.plot.bar()\n",
    "plt.title('Best BOGO model feature importance')\n",
    "plt.show()\n",
    "\n",
    "#discount 3 model\n",
    "feature_importances = pd.DataFrame(discount_3.feature_importances_,\n",
    "                                   index = features.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "feature_importances.plot.bar()\n",
    "plt.title('Best discount model feature importance')\n",
    "plt.show()\n",
    "\n",
    "#info_2 model\n",
    "#prepare data similar to info_2 state\n",
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type']\n",
    "features,target=data_prep(offers_discount,drop_cols_prep)\n",
    "#print feature importance\n",
    "feature_importances = pd.DataFrame(info_2.feature_importances_,\n",
    "                                   index = features.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "feature_importances.plot.bar()\n",
    "plt.title('Best info model feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking on the feature importance to analyse the main drivers of an effective offer, we can see that the most important driver of effective offers across all three are the tenure of membership. However, the 2nd most important feature is different for each of the three models.\n",
    "\n",
    "For a BOGO offer, the membership tenure is the most important feature, and the other variables are a lot smaller in proportions. Income, age and offer_received_cnt are the 2nd, 3rd and 4th most important features, but their proportions are very small.\n",
    "\n",
    "For a discount offer, after the membership tenure, age and income are the next most important variables. But it is still very small in proportions.\n",
    "\n",
    "The feature importances for the informational offer models are more distributed compared to the BOGO and discount models, with income being the 2nd most important feature. Age is the third and mobile channel interestingly being the 4th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Exploration on users in Groups 3 and 4 - People who purchase regardless of viewing any offers\n",
    "\n",
    "We had earlier delineated those in groups 3 and 4 as people who would purchase regardless of viewing any offers. Now we can do some exploratory analyses to see what kind of demographic this group of users consist of.\n",
    "\n",
    "**c.i. Data Preparation:**\n",
    "\n",
    "It would be interesting to see how people in groups 3 and 4 contrast with people in groups 1 and 2, so I decided to compare between all 3.\n",
    "\n",
    "First, I need to append the data from all groups from the three offer types together, then compare the characteristics of each group via visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append datasets together\n",
    "\n",
    "#grp 3+4\n",
    "grp3_4=grp3_bogo.append(grp3_discount,sort=False)\n",
    "grp3_4=grp3_4.append(grp3_info,sort=False)\n",
    "grp3_4=grp3_4.append(grp4_bogo,sort=False)\n",
    "grp3_4=grp3_4.append(grp4_discount,sort=False)\n",
    "grp3_4=grp3_4.append(grp4_info,sort=False)\n",
    "\n",
    "#grp1\n",
    "grp1_all=grp1_bogo.append(grp1_discount,sort=False)\n",
    "grp1_all=grp1_all.append(grp1_info,sort=False)\n",
    "\n",
    "#grp2\n",
    "grp2_all=grp2_bogo.append(grp2_discount,sort=False)\n",
    "grp2_all=grp2_all.append(grp2_info,sort=False)\n",
    "\n",
    "#get unique person-offer_id pairs\n",
    "grp3_4=grp3_4[['person','offer_id']].groupby(['person','offer_id']).count().reset_index()\n",
    "grp1_all=grp1_all[['person','offer_id']].groupby(['person','offer_id']).count().reset_index()\n",
    "grp2_all=grp2_all[['person','offer_id']].groupby(['person','offer_id']).count().reset_index()\n",
    "\n",
    "#get membership_tenure_days\n",
    "grp3_4=member(grp3_4)\n",
    "grp1_all=member(grp1_all)\n",
    "grp2_all=member(grp2_all)\n",
    "\n",
    "#merge with transcript to check transaction amount\n",
    "grp3_4=grp3_4.merge(transcript[['person','offer_id','amount']].groupby(['person','offer_id']).sum(),on=['person','offer_id'],how='left')\n",
    "grp1_all=grp1_all.merge(transcript[['person','offer_id','amount']].groupby(['person','offer_id']).sum(),on=['person','offer_id'],how='left')\n",
    "grp2_all=grp2_all.merge(transcript[['person','offer_id','amount']].groupby(['person','offer_id']).sum(),on=['person','offer_id'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also clean the dataset of null values, similar to the preparation of the datasets above for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check null values\n",
    "print(\"For grp 3 and 4:\")\n",
    "print((grp3_4.isnull().sum()/len(grp3_4))*100)\n",
    "\n",
    "#drop null values\n",
    "grp3_4=grp3_4.dropna()\n",
    "\n",
    "#check null values\n",
    "print(\"For grp 1:\")\n",
    "print((grp1_all.isnull().sum()/len(grp1_all))*100)\n",
    "\n",
    "#drop null values\n",
    "grp1_all=grp1_all.dropna()\n",
    "\n",
    "#check null values\n",
    "print(\"For grp 2:\")\n",
    "print((grp2_all.isnull().sum()/len(grp2_all))*100)\n",
    "\n",
    "#drop null values\n",
    "grp2_all=grp2_all.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check size of groups\n",
    "print(\"Size of group 1: \"+ str(len(grp1_all['person'])))\n",
    "print(\"Size of group 3+4: \"+ str(len(grp3_4['person'])))\n",
    "print(\"Size of group 2: \"+ str(len(grp2_all['person'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the sizes of the 3 groups, we can see that group 1 is the largest, while group 2 is the smallest, which is unsurprising as we had seen that the classes in our datasets were imbalanced in favour of positive classes (i.e. `effective_offers=1`). Meanwhile for people in groups 3 and 4 there are quite a significant number of people as well, larger than the number of people in group 2.\n",
    "\n",
    "**c.ii. Exploration of demographic characteristics:**\n",
    "\n",
    "Meanwhile, in order to effectively compare between the groups, I created a function to efficiently visualize the groups together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function for plotting multiple histograms overlaying the 3 groups\n",
    "def plot_hist(variable,bins=None):\n",
    "    plt.hist(grp1_all[variable],alpha=0.5, label='group 1',bins=bins)\n",
    "    plt.hist(grp3_4[variable], alpha=0.5, label='group 3 and 4',bins=bins)\n",
    "    plt.hist(grp2_all[variable], alpha=0.5, label='group 2',bins=bins)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('distribution of '+ variable + ' between group 1, group 2 and groups 3 + 4')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can explore the income distribution between the 3 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot distribution of income\n",
    "plot_hist('income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the 3 segments, most people fall within the middle range of income (50K - 100K). The income distribution between the 3 segments are relatively similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot ditribution of age\n",
    "plot_hist('age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age distribution looks relatively similar between the 3 groups as well, with most people between the age 40-80 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot distribution of amount spent given an effective offer\n",
    "plot_hist('amount',bins=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 2 are people who did not spend at all as the offers were ineffective on them, hence they are not in the graph. But for groups 1 and 3+4, we can see that the amount spent is relatively similar, except that people in group 1 spent slightly more. This is to be expected as we might expect that the offers managed to incentivise them to purchase more, hence their overall spend increased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot tenure of membership\n",
    "plot_hist('membership_tenure_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of membership tenure also looks similar between the 3 segments, with most people between 0-700 days of tenure. It appears as though there are not much demographic characteristic differences between the 3 groups, at least in the current data provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Potential all-in-one model\n",
    "\n",
    "Out of curiosity, I wondered if we could predict the effectiveness of an offer if the offer type was included as a categorical feature. Would the type of offer affect the user's responsiveness?\n",
    "\n",
    "To do this, I would need to do some minor data preparation to prepare the data for a multiclass model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append datasets together\n",
    "offers_bogo['offer_type']='bogo'\n",
    "offers_info['offer_type']='informational'\n",
    "offers_discount['offer_type']='discount'\n",
    "offers=offers_discount.append(offers_bogo,sort=False)\n",
    "offers=offers.append(offers_info,sort=False)\n",
    "\n",
    "#create dummy variable for offer_type categorical variable\n",
    "offers=dummy(offers,'offer_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do grid search to find optimal parameters for RF model\n",
    "drop_cols_prep=['person','offer_id','effective_offer','amount_invalid']\n",
    "features,target=data_prep(offers,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "rand_forest_param_selection(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols_prep=['person','offer_id','effective_offer','amount_invalid']\n",
    "features,target=data_prep(offers,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "all_in_one = RandomForestClassifier(random_state=5,criterion='gini',max_depth= 20, max_features= 'auto',min_samples_split= 2,n_estimators=50,min_samples_leaf=15)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,all_in_one,'all_in_one')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing best performance of all 3 models with all_in_one model\n",
    "results[['RandomForestClassifier_bogo_3','RandomForestClassifier_discount_3','RandomForestClassifier_info_2','DecisionTreeClassifier_all_in_one','RandomForestClassifier_all_in_one']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[['testing_score'],['RandomForestClassifier_bogo_3','RandomForestClassifier_discount_3','RandomForestClassifier_info_2','DecisionTreeClassifier_all_in_one','RandomForestClassifier_all_in_one']].plot.bar()\n",
    "plt.title('Comparing testing set accuracy score for the 3 models vs all-in-one model')\n",
    "plt.legend(loc=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the performance of the 3 best models for each offer type with the all_in_one model, we can se that having the all-in-one model is not as good as the RF bogo and discount models, and is about slightly better than the info model. This is probably due to the info model pulling down the performance, resulting in lower accuracy for the all in one model. I suspect that if we were to break down the all-in-one model performance to just looking at its ability to predict the effectiveness of informational offer types, it would also be worse than its performance predicting the other 2 types.  \n",
    "\n",
    "If we take a step back and look at the big picture, it is more useful to have a higher accuracy for 3 separate models, as opposed to one all-in-one model. This is because the BOGO and discount offers are actually aimed at driving sales with some promotional cost, whereas the informational offer is essentially 'free' with no cost, and if they can drive sales that would be a bonus.\n",
    "\n",
    "Hence, I would actually suggest that the 3 separate models are more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Given an effective offer, can we predict how much someone would spend? \n",
    "\n",
    "In addition to the all-in-one model, since we already kept the datasets of effective transactions, I was curious to know if I could build a regression model to predict how much someone would spend, given an effective offer. I could have built a model separately for each offer type to predict their spend, but I was curious to know if the type of offer would also determine a user's level of spend. \n",
    "\n",
    "To do this, we have already assigned effective offers based on group 1 customers. From there, we just need to sum up their amount of spend driven by offers to see if we can predict how much someone would spend depending on the offer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append all 3 datasets together\n",
    "grp1=grp1_bogo.append(grp1_discount,sort=False)\n",
    "grp1=grp1.append(grp1_info,sort=False)\n",
    "\n",
    "#drop unnecessary columns\n",
    "drop_cols('effective_offer',grp1,inplace=True)\n",
    "\n",
    "#get offer details\n",
    "grp1=grp1.merge(portfolio,how='left',on='offer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only take into account transactions that are influenced by an offer (i.e. `valid_completed=1`) as we want to predict the spend given (i.e. based on) the influence of an effective offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get sum of valid transactions per person based on unique person and offer_id pair\n",
    "grp1=grp1.merge(transcript[['person','offer_id','amount']][transcript['valid_completed']==1].groupby(['person','offer_id']).sum(),on=['person','offer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get demographic data and membership_tenure details\n",
    "grp1=member(grp1)\n",
    "\n",
    "#reset index for offers_info\n",
    "grp1=drop_cols('index',grp1.reset_index())\n",
    "\n",
    "#reuse offers_info channel_col function to expand channel column into categorical variables\n",
    "channel_col('web',grp1)\n",
    "channel_col('email',grp1)\n",
    "channel_col('mobile',grp1)\n",
    "channel_col('social',grp1);\n",
    "\n",
    "drop_cols('channels',grp1,inplace=True);\n",
    "\n",
    "#reuse offers_info function to prep dataset\n",
    "grp1=prep_offers_df(grp1)\n",
    "\n",
    "#encode offer type as dummy variables\n",
    "grp1=dummy(grp1,'offer_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a regression model, we need to prevent multicollinearity by reducing the level of the dummy variables by 1, dropping those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add one level of dummy variable to drop\n",
    "drop_cols_prep=['person', 'offer_id','amount','social','gender_O','offer_type_informational']\n",
    "target=grp1['amount']\n",
    "features=drop_cols(drop_cols_prep,grp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a regression model, we need to change the metrics such that it is not a classification model. Hence, I tweak my `train_predict` and `run_model` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweak train_predict function -\n",
    "def train_predict_reg(learner, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: review_scores_rating training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: review_scores_rating testing set\n",
    "    '''\n",
    "    results = {}\n",
    "    \n",
    "    #Fit the learner to the training data and get training time\n",
    "    start = time() \n",
    "    learner = learner.fit(X_train, y_train)\n",
    "    end = time() \n",
    "    results['train_time'] = end-start\n",
    "    \n",
    "    # Get predictions on the test set(X_test), then get predictions on first 300 training samples\n",
    "    start = time() \n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train)\n",
    "    end = time() \n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end-start\n",
    "    \n",
    "    #add training accuracy to results\n",
    "    results['training_score']=learner.score(X_train,y_train)\n",
    "    \n",
    "    #add testing accuracy to results\n",
    "    results['testing_score']=learner.score(X_test,y_test)\n",
    "    \n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, len(y_train)))\n",
    "    print(\"MSE_train: %.4f\" % mean_squared_error(y_train,predictions_train))\n",
    "    print(\"MSE_test: %.4f\" % mean_squared_error(y_test,predictions_test))\n",
    "    print(\"Training accuracy:%.4f\" % results['training_score'])\n",
    "    print(\"Test accuracy:%.4f\" % results['testing_score'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_reg(clf1,clf2,name):\n",
    "    '''\n",
    "    input:\n",
    "    - clf1: baseline regression model\n",
    "    - clf2: 2nd regression model to compare\n",
    "    - name: name to keep track of comparison\n",
    "    output:\n",
    "    - dataframe containing results of training and prediction of model\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Collect results on the learners\n",
    "    results = {}\n",
    "    for clf in [clf1, clf2]:\n",
    "        clf_name = clf.__class__.__name__ + '_' +name\n",
    "        results[clf_name] = {}\n",
    "        results[clf_name]= train_predict_reg(clf, X_train, y_train, X_test, y_test)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=model_pipeline_poly(features,target,2)\n",
    "\n",
    "#Initialize the model\n",
    "clf1 = Ridge(alpha=2,random_state=2)\n",
    "clf2 = DecisionTreeRegressor(random_state=2)\n",
    "\n",
    "results_reg=run_model_reg(clf1,clf2,'reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression models really underperformed in terms of predicting the amount spent. It appears with the current data within our group 1 of customers, there is not enough information to predict the amount that can be driven by the offer type. We can see the Decision Tree Regressor model really overfit the data, with a very high training score but sub par testing score. Meanwhile, the linear regression model (with ridge/l2 regularization) also shows a minimal correlation between the features and the target variable. The model really underfits the data.\n",
    "\n",
    "I may get better performance if I break the models up into 3 different models based on offer type again; or even try to include non-influenced/invalid transactions, but this could be an exploration for another time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Overall, I found this project challenging, mainly due to the structure of the data in the `transcript` dataset. I had started out with 2 business questions:\n",
    "\n",
    "1. What are the main drivers of an effective offer on the Starbucks app?\n",
    "2. Could the data provided, namely offer characteristics and user demographics, predict whether a user would take up an offer?\n",
    "\n",
    "### a. Reflection:\n",
    "\n",
    "#### a.i. Question 1 findings:\n",
    "For Question 1, the feature importance given by all 3 models were that the tenure of a member is the biggest predictor of the effectiveness of an offer. Further study would be able to indicate what average tenure days would result in an effective BOGO offer. \n",
    "\n",
    "For all three models, the top 3 variables were the same - membership tenure, income and age. However, income and age switched orders depending on offer type. \n",
    "\n",
    "For BOGO and discount offers, the distribution of feature importances were relatively equal. However, for informational offers, the distribution is slightly more balanced, with income the second most important variable.\n",
    "\n",
    "#### a.ii. Question 2 findings:\n",
    "\n",
    "My decision to use 3 separate models to predict the effectiveness of each offer type ended up with good accuracy for the BOGO and discount models (82.83% for BOGO and 87.35% for discount), while slightly less accurate performance for informational offers (75.3%). However, I would regard 75% as acceptable in a business setting, as for informational offers, there is no cost involved to inform users of a product.\n",
    "\n",
    "Meanwhile, for BOGO and discount models, I am quite happy with the 80% and above accuracy, as in a business setting that would be acceptable to show offers to people, even if the model misclassifies a few, the overall revenue increase might justify the few mistakes. \n",
    "\n",
    "### b. Main challenges and potential improvement:\n",
    "\n",
    "When analysing and building the machine learning models to answer the above questions, reflections on my main challenges and findings are as follows:\n",
    "\n",
    "#### b.i. Attribution framework for assigning offer_ids for transactions:\n",
    "\n",
    "In order to answer Question 1, I had to first define what an 'effective offer' means using the transactional records. This proved to be the trickiest portion of the project. I had to define a funnel for what what an effective conversion would look like, as we had data on both effective and noneffective conversions. Thus, I was desigining an attribution model for the conversion events (`offer completed` and `transaction` events) based on the events that occurred prior for each person.\n",
    "\n",
    "I ended up having to separate the users into 4 different pools, based on their actions in the transcript data:\n",
    "\n",
    "- Group 1: People who are influenced by offers and thus purchase/complete the offer(successful/effective conversion of offer)\n",
    "- Group 2: People who receive and an offer but is not influenced and thus no conversion event (ineffective conversion of offer)\n",
    "- Group 3: People who have conversion events but was not actually influenced by an offer\n",
    "- Group 4: People who receive offers but no views or action taken\n",
    "\n",
    "Even after separating the groups, it was challenging to assign the people in group 3 based on the transactional data. I had to define the event space where the right sequence of events would occur before I could assign an offer id to transactions (which did not have an offer_id), essentally designing a event/sequence-based attribution window.\n",
    "\n",
    "After attributing the conversions to specific offers, the rest of the data preparation and cleaning was relatively straightforward. I was grateful that there were not many missing values, and the preparation of categorical variables was also relatively straightforward.\n",
    "\n",
    "#### b.ii. Feature engineering:\n",
    "\n",
    "I decided to do some basic feature engineering as I found the model had slightly underfit on my first attempt in this project, so I had added the feature engineering section later. It improved the performance of the model slightly, and the membership_tenure feature I had engineered out of the `became_member_on` column ended up being the most important predictor variable.\n",
    "\n",
    "However, overall I found that I could not think of additional features using the time data, even though I had the hunch that the time of receiving the offer might be quite influential in determining whether it is effective or not. \n",
    "\n",
    "#### b.iii. Model implementation decisions:\n",
    "\n",
    "I had made the decision to build 3 separate models depending on offer types based on my definition of the problem statement - as I wanted to discover what would drive an effective offer, I thought it made more sense to remove noise from the data by separating the data into the offer types. My decision ended up to be quite a good one as the single BOGO and discount models got good performance in testing scores, compared to the all-in-one model overall score. \n",
    "\n",
    "For the info model, the accuracy was slightly worse as we had less records overall (half of the BOGO and discount models). As elaborated above, I believe that if we had more data, I could have gotten the accuracy higher, as there was a clear diverging pattern occurring between the training and testing score as I made decisions to improve the model fit like adding polynomial features and removing 'noisy' features like the amount_invalid feature. Due to the limited data, my decisions ended up with the model overfitting, hence I believe the model accuracy would have benefitted from more data.\n",
    "\n",
    "An additional note on model selection - I selected tree-based models as I wanted to assess feature importance, but I could have extended this study further by testing a parametric/ regression model (e.g. logistic regression for classification tasks). The weights of the coefficients from a regression model might have been interesting to contrast with the feature importance of a tree-based model, given that both models have different ways of analysing the data. The feature `membership_tenure_days` might not have been the highest weighted feature, in contrast to how it was in this study.\n",
    "\n",
    "#### b.iv. Exploring demographics of different customer groups:\n",
    "\n",
    "I was curious to know what the characteristics were of groups 3 and 4, which are customers who are not influenced by an offer at all. However, after comparing their characteristics with groups 1 and 2, I could not see any significant differences in their demographics.\n",
    "\n",
    "I would have liked to have more data to perhaps understand why this group of customers tend to not be influenced by offer, in order to make useful suggestions on how to give a good customer experience to these customers, even if we do not serve them any offers.\n",
    "\n",
    "#### b.v. Model accuracy in predicting amount spent given an effective offer:\n",
    "\n",
    "The regression model I built out of curiosity to see if we could predict the amount a user would spend, given that they are effectively influenced by an offer. The motivation was that if we can predict how much someone would spend given an offer, perhaps we can assess which offers bring in the most revenue. \n",
    "\n",
    "However, my model found virtually no correlation between the features provided (namely, offer characteristics and demographics of app users) with the amount spent per user. These features aren't strong enough to predict the amount spent per user. Perhaps if we also have a value of the offer, for example, for a discount offer, the value of the discount in dollar terms, perhaps we might be able to predict better.\n",
    "\n",
    "Perhaps I could have broken them up into 3 different models for the 3 offer types, the way I did with the binary classification models, in order to get a better result. However, given that this was just a curiosity and I wanted to explore if the offer type would be a statistically significant predictor feature, I built an all-in-one model for this instance. This would be worth exploring further, given more time and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thanks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
